{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f8c2c",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n\nPROJ_ROOT = Path.cwd().parent\nif str(PROJ_ROOT) not in sys.path:\n    sys.path.append(str(PROJ_ROOT))\n\nfrom credit_risk_xai.config import FEATURE_CACHE_PATH, MIN_REVENUE_KSEK\nfrom credit_risk_xai.features.engineer import prepare_modeling_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cbf63",
   "metadata": {},
   "outputs": [],
   "source": "# Load and filter data\ndf = pd.read_parquet(FEATURE_CACHE_PATH)\nfiltered = df[(df[\"ser_aktiv\"] == 1) & (df[\"rr01_ntoms\"] >= MIN_REVENUE_KSEK)]\nX, y = prepare_modeling_data(filtered)\n\nprint(f\"Features: {X.shape[1]} | Samples: {len(X):,}\")\nprint(f\"Target distribution:\\n{y.value_counts()}\")\nprint(f\"Imbalance: {(y==0).sum()/(y==1).sum():.1f}:1\")"
  },
  {
   "cell_type": "code",
   "id": "ilr63x4ki4k",
   "source": "# Train/val split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nprint(f\"Train: {len(X_train):,} | Val: {len(X_val):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dl9w9d98jqe",
   "source": "# Train LightGBM\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\nmodel = lgb.LGBMClassifier(\n    n_estimators=10000,\n    learning_rate=0.05,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    scale_pos_weight=scale_pos_weight,\n    random_state=42,\n    n_jobs=-1,\n    verbose=-1\n)\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_train, y_train), (X_val, y_val)],\n    eval_metric='auc',\n    callbacks=[lgb.early_stopping(50, verbose=True), lgb.log_evaluation(100)]\n)\n\nprint(f\"\\nBest iteration: {model.best_iteration_}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "htjbzby6qel",
   "source": "# Evaluate\ny_pred = model.predict(X_val)\ny_pred_proba = model.predict_proba(X_val)[:, 1]\n\nauc = roc_auc_score(y_val, y_pred_proba)\npr_auc = average_precision_score(y_val, y_pred_proba)\n\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"PR-AUC: {pr_auc:.4f}\\n\")\nprint(\"Classification Report:\")\nprint(classification_report(y_val, y_pred))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_val, y_pred))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yijb3okzet",
   "source": "# Feature importance\nimportance_df = pd.DataFrame({\n    'feature': model.feature_name_,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 20 Features:\")\nprint(importance_df.head(20).to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aymh1btlp4",
   "source": "# SHAP analysis (optional - uncomment to run)\n# import shap\n# sample_size = min(5000, len(X_val))\n# X_sample = X_val.sample(n=sample_size, random_state=42)\n# explainer = shap.TreeExplainer(model)\n# shap_values = explainer.shap_values(X_sample)\n# if isinstance(shap_values, list):\n#     shap_values = shap_values[1]\n# shap.summary_plot(shap_values, X_sample)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}