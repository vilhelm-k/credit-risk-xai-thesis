{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Default Archetype Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n\nimport umap\n\nPROJ_ROOT = Path.cwd().parent\nif str(PROJ_ROOT) not in sys.path:\n    sys.path.append(str(PROJ_ROOT))\n\nfrom credit_risk_xai.config import ACTIVE_MODEL_VERSION, ACTIVE_FEATURES, get_display_name\nfrom credit_risk_xai.plotting import (\n    set_thesis_style, COLORS, FIGSIZE, save_figure, despine,\n)\n\n# Initialize thesis-quality plotting style\nset_thesis_style(use_tex=True)\n\nFIGURES_DIR = PROJ_ROOT / \"figures\"\n\nprint(f\"Model version: {ACTIVE_MODEL_VERSION} ({len(ACTIVE_FEATURES)} features)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SHAP Cache and Filter to Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = PROJ_ROOT / \"results\" / \"xai_exploration\" / \"shap_cache.pkl\"\n",
    "with open(cache_file, 'rb') as f:\n",
    "    shap_cache = pickle.load(f)\n",
    "\n",
    "X_val = shap_cache['X_val']\n",
    "y_val = shap_cache['y_val']\n",
    "y_pred_lgbm = shap_cache['y_pred_proba_lgbm']\n",
    "y_pred_logit = shap_cache['y_pred_proba_logit']\n",
    "shap_values_lgbm = shap_cache['shap_values_lgbm']\n",
    "feature_names = shap_cache['feature_names']\n",
    "\n",
    "DEFAULTS_ONLY = False  # Toggle: True = defaults only, False = all samples\n",
    "\n",
    "if DEFAULTS_ONLY:\n",
    "    default_mask = y_val.values == 1\n",
    "else:\n",
    "    default_mask = np.ones(len(y_val), dtype=bool)  # All True\n",
    "    \n",
    "X_defaults = X_val[default_mask]\n",
    "shap_defaults = shap_values_lgbm[default_mask]\n",
    "pd_defaults_lgbm = y_pred_lgbm[default_mask]\n",
    "pd_defaults_logit = y_pred_logit[default_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing for Clustering\n",
    "\n",
    "Steps:\n",
    "1. **Standardize SHAP values** - Scale to unit variance\n",
    "2. **PCA** - Reduce dimensionality while preserving 95% variance\n",
    "3. **UMAP** - Create 2D embedding for visualization\n",
    "\n",
    "Note: With ~1,000 defaults (vs 60,000 total firms), we can use faster settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "shap_scaled = scaler.fit_transform(shap_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "shap_pca = pca.fit_transform(shap_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "axes[0].plot(range(1, len(cumvar)+1), cumvar, 'bo-')\n",
    "axes[0].axhline(y=0.95, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Components')\n",
    "axes[0].set_ylabel('Cumulative Variance')\n",
    "\n",
    "axes[1].bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, color='steelblue')\n",
    "axes[1].set_xlabel('Component')\n",
    "axes[1].set_ylabel('Variance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "shap_2d = reducer.fit_transform(shap_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Determine Optimal Number of Clusters\n",
    "\n",
    "With ~1,000 defaults, we test k=2 to 6 (fewer clusters than before due to smaller sample size).\n",
    "\n",
    "Evaluation metrics:\n",
    "- **Elbow method**: Inertia (within-cluster sum of squares)\n",
    "- **Silhouette score**: Measure of cluster cohesion and separation\n",
    "- **Davies-Bouldin index**: Ratio of within-cluster to between-cluster distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_range = range(2, 7)\n",
    "results = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(shap_pca)\n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'silhouette': silhouette_score(shap_pca, labels),\n",
    "        'davies_bouldin': davies_bouldin_score(shap_pca, labels)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "HTML(results_df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(results_df['k'], results_df['inertia'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "axes[0].set_ylabel('Inertia', fontsize=11)\n",
    "axes[0].set_title('Elbow Method', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "best_k_silhouette = int(results_df.loc[results_df['silhouette'].idxmax(), 'k'])\n",
    "axes[1].plot(results_df['k'], results_df['silhouette'], 'go-', linewidth=2, markersize=8)\n",
    "axes[1].axvline(x=best_k_silhouette, color='r', linestyle='--', label=f'Best k={best_k_silhouette}')\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[1].set_title('Silhouette Score (higher is better)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "best_k_db = int(results_df.loc[results_df['davies_bouldin'].idxmin(), 'k'])\n",
    "axes[2].plot(results_df['k'], results_df['davies_bouldin'], 'ro-', linewidth=2, markersize=8)\n",
    "axes[2].axvline(x=best_k_db, color='g', linestyle='--', label=f'Best k={best_k_db}')\n",
    "axes[2].set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "axes[2].set_ylabel('Davies-Bouldin Index', fontsize=11)\n",
    "axes[2].set_title('Davies-Bouldin Index (lower is better)', fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_optimal = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering of Default Archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_final = KMeans(n_clusters=k_optimal, random_state=42, n_init=10, max_iter=300)\n",
    "cluster_labels = kmeans_final.fit_predict(shap_pca)\n",
    "\n",
    "cluster_df = pd.DataFrame({\n",
    "    'cluster': cluster_labels,\n",
    "    'pd_lgbm': pd_defaults_lgbm,\n",
    "    'pd_logit': pd_defaults_logit\n",
    "}, index=X_defaults.index)\n",
    "\n",
    "cluster_df['umap_x'] = shap_2d[:, 0]\n",
    "cluster_df['umap_y'] = shap_2d[:, 1]\n",
    "\n",
    "cluster_sizes = cluster_df['cluster'].value_counts().sort_index()\n",
    "HTML(cluster_sizes.to_frame().to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Default Archetype Characterization\n",
    "\n",
    "For each archetype, we analyze:\n",
    "1. Predicted PD distribution (how \"expected\" were these defaults?)\n",
    "2. SHAP value profile (which features drove default in this group?)\n",
    "3. Distinguishing characteristics (what makes this archetype unique?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summary = cluster_df.groupby('cluster').agg({\n",
    "    'pd_lgbm': ['count', 'mean', 'median', 'std', 'min', 'max'],\n",
    "    'pd_logit': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "cluster_summary.columns = ['Size', 'Mean_PD_LightGBM', 'Median_PD_LightGBM', \n",
    "                           'Std_PD_LightGBM', 'Min_PD_LightGBM', 'Max_PD_LightGBM',\n",
    "                           'Mean_PD_Logit']\n",
    "\n",
    "cluster_summary['Pct_of_Defaults'] = (cluster_summary['Size'] / len(cluster_df) * 100).round(1)\n",
    "HTML(cluster_summary.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sort archetypes by mean predicted PD\n",
    "cluster_order = cluster_summary.sort_values('Mean_PD_LightGBM').index.tolist()\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, k_optimal))\n",
    "\n",
    "# 1. Mean predicted PD bar chart\n",
    "sorted_pds = cluster_summary.loc[cluster_order, 'Mean_PD_LightGBM']\n",
    "bars = axes[0].bar(range(k_optimal), sorted_pds * 100, color=colors)\n",
    "axes[0].axhline(y=cluster_df['pd_lgbm'].mean()*100, color='black', linestyle='--', \n",
    "                linewidth=2, label=f'Overall mean: {cluster_df[\"pd_lgbm\"].mean()*100:.2f}%')\n",
    "axes[0].set_xticks(range(k_optimal))\n",
    "axes[0].set_xticklabels([f'A{c}' for c in cluster_order])\n",
    "axes[0].set_xlabel('Default Archetype', fontsize=11)\n",
    "axes[0].set_ylabel('Mean Predicted PD (%)', fontsize=11)\n",
    "axes[0].set_title('Mean Predicted PD by Archetype', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "for bar, pd_val in zip(bars, sorted_pds):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{pd_val*100:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 2. Archetype sizes\n",
    "sorted_sizes = cluster_summary.loc[cluster_order, 'Size']\n",
    "bars2 = axes[1].bar(range(k_optimal), sorted_sizes, color=colors)\n",
    "axes[1].set_xticks(range(k_optimal))\n",
    "axes[1].set_xticklabels([f'A{c}' for c in cluster_order])\n",
    "axes[1].set_xlabel('Default Archetype', fontsize=11)\n",
    "axes[1].set_ylabel('Number of Defaults', fontsize=11)\n",
    "axes[1].set_title('Archetype Sizes', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "for bar, size in zip(bars2, sorted_sizes):\n",
    "    pct = size / len(cluster_df) * 100\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{size:,}\\n({pct:.0f}%)', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('FIGURE 2: Default Archetype Summary', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df = pd.DataFrame(shap_defaults, columns=feature_names, index=X_defaults.index)\n",
    "shap_df['cluster'] = cluster_labels\n",
    "\n",
    "cluster_shap_means = shap_df.groupby('cluster')[feature_names].mean()\n",
    "overall_default_shap = shap_df[feature_names].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Sort features by overall importance (mean absolute SHAP across defaults)\n",
    "feature_importance = np.abs(shap_defaults).mean(axis=0)\n",
    "sorted_features = [feature_names[i] for i in np.argsort(feature_importance)[::-1]]\n",
    "\n",
    "# Reorder columns\n",
    "cluster_shap_sorted = cluster_shap_means[sorted_features]\n",
    "\n",
    "# Sort rows by mean predicted PD\n",
    "cluster_order_by_pd = cluster_summary.sort_values('Mean_PD_LightGBM').index.tolist()\n",
    "cluster_shap_sorted = cluster_shap_sorted.loc[cluster_order_by_pd]\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    cluster_shap_sorted,\n",
    "    annot=False,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Mean SHAP Value'}\n",
    ")\n",
    "\n",
    "# Add PD annotation to y-axis\n",
    "ytick_labels = [f\"A{c} (PD: {cluster_summary.loc[c, 'Mean_PD_LightGBM']*100:.1f}%)\" \n",
    "                for c in cluster_order_by_pd]\n",
    "ax.set_yticklabels(ytick_labels, rotation=0)\n",
    "\n",
    "ax.set_xlabel('Feature (sorted by importance among defaults)', fontsize=11)\n",
    "ax.set_ylabel('Archetype (sorted by mean predicted PD)', fontsize=11)\n",
    "ax.set_title('FIGURE 3: Mean SHAP Values by Default Archetype\\n(Red = increases risk, Blue = decreases risk)',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_descriptions = {}\n",
    "cluster_order_by_pd = cluster_summary.sort_values('Mean_PD_LightGBM').index.tolist()\n",
    "\n",
    "for cluster_id in cluster_order_by_pd:\n",
    "    cluster_shap = cluster_shap_means.loc[cluster_id]\n",
    "    top_positive = cluster_shap.nlargest(5)\n",
    "    top_negative = cluster_shap.nsmallest(3)\n",
    "    \n",
    "    archetype_descriptions[cluster_id] = {\n",
    "        'top_risk_drivers': top_positive.index.tolist()[:3],\n",
    "        'mean_pd': cluster_summary.loc[cluster_id, 'Mean_PD_LightGBM'],\n",
    "        'size': int(cluster_summary.loc[cluster_id, 'Size'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Color by archetype\n",
    "scatter = ax.scatter(\n",
    "    cluster_df['umap_x'], \n",
    "    cluster_df['umap_y'],\n",
    "    c=cluster_df['cluster'],\n",
    "    cmap='tab10',\n",
    "    alpha=0.6,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "# Add archetype centers\n",
    "for c in range(k_optimal):\n",
    "    mask = cluster_df['cluster'] == c\n",
    "    cx = cluster_df.loc[mask, 'umap_x'].mean()\n",
    "    cy = cluster_df.loc[mask, 'umap_y'].mean()\n",
    "    ax.scatter([cx], [cy], c='black', s=300, marker='X', edgecolors='white', linewidths=2, zorder=10)\n",
    "    ax.annotate(f'A{c}', (cx, cy), fontsize=14, fontweight='bold', \n",
    "                ha='center', va='bottom', color='black',\n",
    "                xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('UMAP 1', fontsize=11)\n",
    "ax.set_ylabel('UMAP 2', fontsize=11)\n",
    "ax.set_title('FIGURE 4: Default Archetypes in SHAP Space\\n(Each point is a defaulting firm)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [plt.scatter([], [], c=plt.cm.tab10(i/10), s=50, \n",
    "                               label=f'Archetype {i} (n={int(cluster_summary.loc[i, \"Size\"]):,})')\n",
    "                   for i in range(k_optimal)]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Clusters represent different 'pathways to default' based on SHAP profiles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Archetype Deep Dive: PD Distributions\n",
    "\n",
    "How \"expected\" were these defaults? Archetypes with low predicted PD represent \"surprise\" defaults that the model failed to anticipate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = min(k_optimal, 3)\n",
    "n_rows = (k_optimal + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "if k_optimal > 1:\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "else:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, cluster_id in enumerate(cluster_order_by_pd):\n",
    "    ax = axes[i]\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "    \n",
    "    # Histogram of PDs\n",
    "    ax.hist(cluster_data['pd_lgbm'] * 100, bins=30, color='firebrick', \n",
    "            alpha=0.7, edgecolor='white')\n",
    "    \n",
    "    # Add vertical lines for mean and median\n",
    "    mean_pd = cluster_data['pd_lgbm'].mean() * 100\n",
    "    median_pd = cluster_data['pd_lgbm'].median() * 100\n",
    "    ax.axvline(x=mean_pd, color='black', linestyle='-', linewidth=2, label=f'Mean: {mean_pd:.1f}%')\n",
    "    ax.axvline(x=median_pd, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_pd:.1f}%')\n",
    "    \n",
    "    size = int(cluster_summary.loc[cluster_id, 'Size'])\n",
    "    ax.set_xlabel('Predicted PD (%)', fontsize=10)\n",
    "    ax.set_ylabel('Count', fontsize=10)\n",
    "    ax.set_title(f'Archetype {cluster_id}\\n{size:,} defaults', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle('FIGURE 5: Distribution of Predicted PDs by Default Archetype', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify \"surprise\" defaults\n",
    "low_pd_threshold = 0.05  # 5%\n",
    "surprise_defaults = cluster_df[cluster_df['pd_lgbm'] < low_pd_threshold]\n",
    "print(f\"\\n'Surprise' defaults (PD < {low_pd_threshold*100:.0f}%): {len(surprise_defaults):,} ({len(surprise_defaults)/len(cluster_df)*100:.1f}%)\")\n",
    "print(\"These are firms the model did not expect to default.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Archetype Naming and Interpretation\n",
    "\n",
    "Based on the SHAP profiles, we assign descriptive names to each archetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_profiles = []\n",
    "\n",
    "for cluster_id in range(k_optimal):\n",
    "    cluster_shap = cluster_shap_means.loc[cluster_id]\n",
    "    top_drivers = cluster_shap.nlargest(3).index.tolist()\n",
    "    \n",
    "    mean_pd = cluster_summary.loc[cluster_id, 'Mean_PD_LightGBM']\n",
    "    size = int(cluster_summary.loc[cluster_id, 'Size'])\n",
    "    \n",
    "    if mean_pd > 0.10:\n",
    "        pd_category = \"High PD (expected defaults)\"\n",
    "    elif mean_pd > 0.05:\n",
    "        pd_category = \"Medium PD\"\n",
    "    else:\n",
    "        pd_category = \"Low PD (surprise defaults)\"\n",
    "    \n",
    "    archetype_profiles.append({\n",
    "        'archetype': cluster_id,\n",
    "        'size': size,\n",
    "        'pct_of_defaults': round(size / len(cluster_df) * 100, 1),\n",
    "        'mean_pd_pct': round(mean_pd * 100, 1),\n",
    "        'pd_category': pd_category,\n",
    "        'top_risk_driver_1': top_drivers[0],\n",
    "        'top_risk_driver_2': top_drivers[1],\n",
    "        'top_risk_driver_3': top_drivers[2]\n",
    "    })\n",
    "\n",
    "archetype_profiles_df = pd.DataFrame(archetype_profiles)\n",
    "archetype_profiles_df = archetype_profiles_df.sort_values('mean_pd_pct', ascending=False)\n",
    "print(archetype_profiles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pd = cluster_summary['Mean_PD_LightGBM'].max()\n",
    "min_pd = cluster_summary['Mean_PD_LightGBM'].min()\n",
    "shap_variance = cluster_shap_means.var(axis=0).mean()\n",
    "feature_variance = cluster_shap_means.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total defaults analyzed',\n",
    "        'Number of archetypes',\n",
    "        'Silhouette score',\n",
    "        'PCA dimensions',\n",
    "        'Highest archetype mean PD',\n",
    "        'Lowest archetype mean PD',\n",
    "        'PD spread',\n",
    "        'Mean cross-archetype SHAP variance'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{len(cluster_df):,}\",\n",
    "        k_optimal,\n",
    "        f\"{results_df[results_df['k']==k_optimal]['silhouette'].values[0]:.4f}\",\n",
    "        f\"{shap_defaults.shape[1]} \u2192 {shap_pca.shape[1]}\",\n",
    "        f\"{max_pd*100:.1f}%\",\n",
    "        f\"{min_pd*100:.1f}%\",\n",
    "        f\"{(max_pd - min_pd)*100:.1f} pp\",\n",
    "        f\"{shap_variance:.4f}\"\n",
    "    ]\n",
    "})\n",
    "HTML(summary_df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = PROJ_ROOT / \"results\" / \"xai_clustering\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cluster_df.to_csv(results_dir / \"cluster_assignments.csv\")\n",
    "cluster_summary.to_csv(results_dir / \"cluster_summary.csv\")\n",
    "cluster_shap_means.to_csv(results_dir / \"cluster_shap_profiles.csv\")\n",
    "archetype_profiles_df.to_csv(results_dir / \"cluster_archetypes.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}