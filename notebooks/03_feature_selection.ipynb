{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0368577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "\n",
    "PROJ_ROOT = Path.cwd().parent\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJ_ROOT))\n",
    "\n",
    "from credit_risk_xai.modeling.train import DEFAULT_PARAMS\n",
    "\n",
    "from credit_risk_xai.config import FEATURE_CACHE_PATH\n",
    "from credit_risk_xai.features.engineer import prepare_modeling_data\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_calibration_curve(y_true, y_pred_proba, n_bins=100, model_name=\"Model\"):\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_true, y_pred_proba, n_bins=n_bins, strategy='quantile'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=model_name)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Calibration Curve - {model_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ECE (Expected Calibration Error)\n",
    "    ece = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    print(f\"ECE: {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd64164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter data\n",
    "MIN_REVENUE_KSEK = 1_000\n",
    "df = pd.read_parquet(FEATURE_CACHE_PATH)\n",
    "df = df[(df[\"ser_aktiv\"] == 1) & (df[\"sme_category\"].isin([\"Small\", \"Medium\"]))]\n",
    "X, y = prepare_modeling_data(df)\n",
    "\n",
    "print(f\"Features: {X.shape[1]} | Samples: {len(X):,}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "print(f\"Imbalance: {(y==0).sum()/(y==1).sum():.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit_risk_xai.features.engineer import prepare_modeling_data\n",
    "from credit_risk_xai.modeling.train import run_lightgbm_training\n",
    "\n",
    "df = pd.read_parquet(FEATURE_CACHE_PATH)\n",
    "mask = (df.ser_aktiv == 1) & (df.sme_category.isin([\"Small\", \"Medium\"]))  # add any extra filters here\n",
    "X, y = prepare_modeling_data(df.loc[mask])\n",
    "\n",
    "results = run_lightgbm_training(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    dataset_description=\"ser_aktiv==1 & SME∈{Small,Medium}\",  # optional note for W&B\n",
    "    use_wandb=False,\n",
    "    wandb_project=\"credit-risk-xai\",\n",
    "    wandb_run_name=\"lgbm_pre_prune2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model = results[\"model\"]\n",
    "X_train = results[\"X_train\"]\n",
    "X_val = results[\"X_val\"]\n",
    "y_train = results[\"y_train\"]\n",
    "y_val = results[\"y_val\"]\n",
    "y_pred_proba = results[\"y_val_proba\"]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_val, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_val, y_pred_proba)\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "plot_calibration_curve(y_val, y_pred_proba, model_name=\"Predicted Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL FEATURE SELECTION - Setup\n",
    "# Define temporal feature groups for systematic analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"TEMPORAL FEATURE SELECTION ANALYSIS\")\n",
    "print(\"=\" * 90)\n",
    "print(\"\\nObjective: For each (metric, computation_type), find optimal time window\")\n",
    "print(\"Method: Systematic ablation study\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Define all temporal feature groups based on engineered_features.md\n",
    "temporal_feature_groups = {\n",
    "    'revenue': {\n",
    "        'cagr': ['revenue_cagr_3y', 'revenue_cagr_5y'],\n",
    "        'drawdown': ['revenue_drawdown_5y']\n",
    "    },\n",
    "    'assets': {\n",
    "        'cagr': ['assets_cagr_3y', 'assets_cagr_5y']\n",
    "    },\n",
    "    'equity': {\n",
    "        'cagr': ['equity_cagr_3y', 'equity_cagr_5y'],\n",
    "        'drawdown': ['equity_drawdown_5y']\n",
    "    },\n",
    "    'profit': {\n",
    "        'cagr': ['profit_cagr_3y', 'profit_cagr_5y']\n",
    "    },\n",
    "    'operating_margin': {\n",
    "        'trend': ['ny_rormarg_trend_3y', 'ny_rormarg_trend_5y'],\n",
    "        'volatility': ['ny_rormarg_vol_3y', 'ny_rormarg_vol_5y'],\n",
    "        'average': ['ny_rormarg_avg_2y', 'ny_rormarg_avg_5y']\n",
    "    },\n",
    "    'net_margin': {\n",
    "        'trend': ['ny_nettomarg_trend_3y', 'ny_nettomarg_trend_5y'],\n",
    "        'volatility': ['ny_nettomarg_vol_3y', 'ny_nettomarg_vol_5y'],\n",
    "        'average': ['ny_nettomarg_avg_2y', 'ny_nettomarg_avg_5y']\n",
    "    },\n",
    "    'leverage': {\n",
    "        'trend': ['ny_skuldgrd_trend_3y', 'ny_skuldgrd_trend_5y'],\n",
    "        'volatility': ['ny_skuldgrd_vol_3y', 'ny_skuldgrd_vol_5y']\n",
    "    },\n",
    "    'cash_liquidity': {\n",
    "        'trend': ['ratio_cash_liquidity_trend_3y', 'ratio_cash_liquidity_trend_5y'],\n",
    "        'volatility': ['ratio_cash_liquidity_vol_3y'],\n",
    "        'average': ['ratio_cash_liquidity_avg_2y', 'ratio_cash_liquidity_avg_5y']\n",
    "    },\n",
    "    'working_capital': {\n",
    "        'trend': ['dso_days_trend_3y', 'inventory_days_trend_3y', 'dpo_days_trend_3y']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flatten all temporal features\n",
    "all_temporal_features = []\n",
    "for metric, computations in temporal_feature_groups.items():\n",
    "    for comp_type, features in computations.items():\n",
    "        all_temporal_features.extend(features)\n",
    "\n",
    "# Get baseline (non-temporal) features from current feature set\n",
    "baseline_features = [f for f in X_train.columns if f not in all_temporal_features]\n",
    "\n",
    "print(f\"\\nTotal temporal features: {len(all_temporal_features)}\")\n",
    "print(f\"Baseline (non-temporal) features: {len(baseline_features)}\")\n",
    "print(f\"Total features in model: {len(X_train.columns)}\")\n",
    "\n",
    "# Verify all temporal features exist in dataset\n",
    "missing_temporal = [f for f in all_temporal_features if f not in X_train.columns]\n",
    "if missing_temporal:\n",
    "    print(f\"\\n⚠️ Warning: {len(missing_temporal)} temporal features not found in dataset:\")\n",
    "    print(missing_temporal)\n",
    "    # Remove missing features from groups\n",
    "    for metric in temporal_feature_groups:\n",
    "        for comp_type in temporal_feature_groups[metric]:\n",
    "            temporal_feature_groups[metric][comp_type] = [\n",
    "                f for f in temporal_feature_groups[metric][comp_type] \n",
    "                if f in X_train.columns\n",
    "            ]\n",
    "\n",
    "print(\"\\n✓ Temporal feature groups defined\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac32a43",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# NESTED CROSS-VALIDATION FRAMEWORK (5×3)\n# Outer 5-fold: Unbiased evaluation\n# Inner 3-fold: Feature selection decisions (averaged)\n# ============================================================================\n\nfrom sklearn.model_selection import StratifiedKFold\n\nprint(\"=\" * 90)\nprint(\"NESTED CROSS-VALIDATION SETUP\")\nprint(\"=\" * 90)\nprint(\"Structure: 5 outer folds × 3 inner folds = 15 train/val splits per test\")\nprint(\"Purpose: Reduce selection bias and provide unbiased performance estimates\")\nprint(\"-\" * 90)\n\n# Define CV splitters\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ninner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=43)\n\ndef train_and_evaluate_cv(features, X, y, cv_splitter, verbose=False):\n    \"\"\"\n    Train and evaluate model using cross-validation.\n    Returns: mean AUC, std AUC, and list of fold AUCs\n    \"\"\"\n    fold_aucs = []\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y)):\n        X_tr, X_v = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_v = y.iloc[train_idx], y.iloc[val_idx]\n        \n        X_tr_sub = X_tr[features]\n        X_v_sub = X_v[features]\n        \n        model = lgb.LGBMClassifier(**DEFAULT_PARAMS)\n        model.fit(\n            X_tr_sub, y_tr,\n            eval_set=[(X_v_sub, y_v)],\n            eval_metric='logloss',\n            callbacks=[lgb.log_evaluation(0), lgb.early_stopping(50)]\n        )\n        \n        y_pred = model.predict_proba(X_v_sub)[:, 1]\n        auc = roc_auc_score(y_v, y_pred)\n        fold_aucs.append(auc)\n        \n        if verbose:\n            print(f\"    Fold {fold_idx+1}: AUC = {auc:.6f}\")\n    \n    return np.mean(fold_aucs), np.std(fold_aucs), fold_aucs\n\ndef format_auc_with_std(mean, std):\n    \"\"\"Format AUC as mean ± std.\"\"\"\n    return f\"{mean:.6f} ± {std:.4f}\"\n\nprint(\"\\n✓ Cross-validation framework configured\")\nprint(f\"  - Outer CV: {outer_cv.n_splits} folds (unbiased test)\")\nprint(f\"  - Inner CV: {inner_cv.n_splits} folds (feature selection)\")\nprint(\"=\" * 90)\n\n# ============================================================================\n# EXPERIMENT 1: Window Selection (5×3 NESTED CV)\n# For each (metric, computation_type), find optimal time window\n# Selection: Based on inner 3-fold CV (averaged to reduce selection bias)\n# Evaluation: Based on outer 5-fold CV (unbiased performance estimates)\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 90)\nprint(\"EXPERIMENT 1: TIME WINDOW SELECTION (5×3 NESTED CV)\")\nprint(\"=\" * 90)\nprint(\"Testing: 2y vs 3y vs 5y for each (metric, computation_type)\")\nprint(\"Decision threshold: synergy > 0.0005 → keep both windows\")\nprint(\"-\" * 90)\n\n# Combine train and val for nested CV\nX_full = pd.concat([X_train, X_val], axis=0).reset_index(drop=True)\ny_full = pd.concat([y_train, y_val], axis=0).reset_index(drop=True)\n\nprint(f\"Full dataset: {len(X_full):,} samples, {y_full.sum():,} positives ({100*y_full.mean():.2f}%)\")\n\nwindow_selection_results_nested = []\n\nfor metric, computations in temporal_feature_groups.items():\n    for comp_type, features in computations.items():\n        if len(features) < 2:\n            continue\n        \n        print(f\"\\n{metric} - {comp_type}\")\n        print(\"-\" * 70)\n        \n        # Extract windows\n        windows = {}\n        for f in features:\n            if '2y' in f:\n                windows['2y'] = f\n            elif '3y' in f:\n                windows['3y'] = f\n            elif '5y' in f:\n                windows['5y'] = f\n        \n        if len(windows) < 2:\n            print(f\"  Skipped: Only one time window\")\n            continue\n        \n        # Store results across outer folds\n        outer_fold_decisions = []\n        outer_fold_test_aucs = []\n        \n        # OUTER CV LOOP (5 folds for unbiased evaluation)\n        for outer_fold_idx, (outer_train_idx, outer_test_idx) in enumerate(outer_cv.split(X_full, y_full)):\n            X_outer_train = X_full.iloc[outer_train_idx]\n            X_outer_test = X_full.iloc[outer_test_idx]\n            y_outer_train = y_full.iloc[outer_train_idx]\n            y_outer_test = y_full.iloc[outer_test_idx]\n            \n            # INNER CV LOOP (3 folds for feature selection decision)\n            # Test individual windows using inner CV\n            inner_results = {}\n            \n            for window_name, window_feat in windows.items():\n                test_features = baseline_features + [window_feat]\n                mean_auc, std_auc, _ = train_and_evaluate_cv(\n                    test_features, X_outer_train, y_outer_train, inner_cv\n                )\n                inner_results[f'{window_name}_only'] = mean_auc\n            \n            # Test all windows together using inner CV\n            test_features = baseline_features + list(windows.values())\n            mean_auc_all, std_auc_all, _ = train_and_evaluate_cv(\n                test_features, X_outer_train, y_outer_train, inner_cv\n            )\n            inner_results['all'] = mean_auc_all\n            \n            # Make decision based on INNER CV results\n            best_single = max([(k, v) for k, v in inner_results.items() if k != 'all'],\n                             key=lambda x: x[1])\n            synergy = mean_auc_all - best_single[1]\n            decision = 'keep_both' if synergy > 0.0005 else best_single[0].replace('_only', '')\n            \n            outer_fold_decisions.append({\n                'fold': outer_fold_idx,\n                'decision': decision,\n                'synergy': synergy,\n                'inner_cv_results': inner_results\n            })\n            \n            # Evaluate the selected configuration on OUTER TEST SET (unbiased)\n            if decision == 'keep_both':\n                selected_features = baseline_features + list(windows.values())\n            else:\n                selected_features = baseline_features + [windows[decision]]\n            \n            # Train on full outer train, test on outer test\n            model = lgb.LGBMClassifier(**DEFAULT_PARAMS)\n            model.fit(\n                X_outer_train[selected_features], y_outer_train,\n                eval_set=[(X_outer_test[selected_features], y_outer_test)],\n                eval_metric='logloss',\n                callbacks=[lgb.log_evaluation(0), lgb.early_stopping(50)]\n            )\n            test_auc = roc_auc_score(\n                y_outer_test, \n                model.predict_proba(X_outer_test[selected_features])[:, 1]\n            )\n            outer_fold_test_aucs.append(test_auc)\n        \n        # Aggregate results across outer folds\n        decisions_count = {}\n        for fold_res in outer_fold_decisions:\n            dec = fold_res['decision']\n            decisions_count[dec] = decisions_count.get(dec, 0) + 1\n        \n        # Majority vote for final decision\n        final_decision = max(decisions_count.items(), key=lambda x: x[1])[0]\n        \n        # Calculate statistics for test performance\n        mean_test_auc = np.mean(outer_fold_test_aucs)\n        std_test_auc = np.std(outer_fold_test_aucs)\n        se_test_auc = std_test_auc / np.sqrt(len(outer_fold_test_aucs))\n        \n        print(f\"  Decision votes: {decisions_count}\")\n        print(f\"  → Final decision: {final_decision}\")\n        print(f\"  Test AUC: {format_auc_with_std(mean_test_auc, std_test_auc)} (SE: {se_test_auc:.4f})\")\n        \n        # Store results\n        window_selection_results_nested.append({\n            'metric': metric,\n            'computation': comp_type,\n            'final_decision': final_decision,\n            'test_auc_mean': mean_test_auc,\n            'test_auc_std': std_test_auc,\n            'test_auc_se': se_test_auc,\n            'decision_votes': str(decisions_count),\n            'features_to_keep': list(windows.values()) if final_decision == 'keep_both'\n                               else [windows[final_decision]]\n        })\n\n# Convert to DataFrame\nwindow_df = pd.DataFrame(window_selection_results_nested)\n\nprint(\"\\n\" + \"=\" * 90)\nprint(\"EXPERIMENT 1 SUMMARY (NESTED CV)\")\nprint(\"=\" * 90)\ndisplay_cols = ['metric', 'computation', 'final_decision', 'test_auc_mean', 'test_auc_std', 'decision_votes']\nprint(window_df[display_cols].to_string(index=False))\n\n# Save results\nwindow_df.to_csv('temporal_window_selection_nested_cv.csv', index=False)\nprint(\"\\n✓ Saved results to: temporal_window_selection_nested_cv.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41667d",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# EXPERIMENT 2: Computation Type Redundancy (5×3 NESTED CV)\n# For each metric, determine which computation types are necessary\n# Using nested CV to avoid overfitting to specific validation quirks\n# ============================================================================\n\nprint(\"=\" * 90)\nprint(\"EXPERIMENT 2: COMPUTATION TYPE REDUNDANCY (5×3 NESTED CV)\")\nprint(\"=\" * 90)\nprint(\"Testing: Which combinations of computation types are necessary per metric?\")\nprint(\"Selection: Based on inner 3-fold CV (averaged)\")\nprint(\"Evaluation: Based on outer 5-fold CV (unbiased)\")\nprint(\"-\" * 90)\n\nfrom itertools import combinations\n\ncomputation_redundancy_results = []\n\nfor metric, computations in temporal_feature_groups.items():\n    if len(computations) <= 1:\n        # Only one computation type, skip\n        continue\n    \n    print(f\"\\n{metric.upper()}\")\n    print(\"-\" * 70)\n    \n    # Get optimal features from Experiment 1 (window selection)\n    optimal_features = {}\n    for comp_type, features in computations.items():\n        matching = window_df[\n            (window_df['metric'] == metric) & \n            (window_df['computation'] == comp_type)\n        ]\n        \n        if len(matching) > 0:\n            optimal_features[comp_type] = matching.iloc[0]['features_to_keep']\n        else:\n            # No window selection (single window or skipped), use all features\n            optimal_features[comp_type] = features\n    \n    # Generate test configurations (all subsets of computation types)\n    comp_types = list(optimal_features.keys())\n    \n    configs = {}\n    for r in range(1, len(comp_types) + 1):\n        for combo in combinations(comp_types, r):\n            config_name = '+'.join(combo)\n            config_features = []\n            for ct in combo:\n                config_features.extend(optimal_features[ct])\n            configs[config_name] = config_features\n    \n    # Store results across outer folds\n    outer_fold_results = {config_name: [] for config_name in configs}\n    outer_fold_decisions = []\n    \n    # OUTER CV LOOP (5 folds)\n    for outer_fold_idx, (outer_train_idx, outer_test_idx) in enumerate(outer_cv.split(X_full, y_full)):\n        X_outer_train = X_full.iloc[outer_train_idx]\n        X_outer_test = X_full.iloc[outer_test_idx]\n        y_outer_train = y_full.iloc[outer_train_idx]\n        y_outer_test = y_full.iloc[outer_test_idx]\n        \n        # INNER CV LOOP - evaluate each configuration on inner 3-fold CV\n        inner_cv_scores = {}\n        \n        for config_name, config_features in configs.items():\n            test_features = baseline_features + config_features\n            mean_auc, _, _ = train_and_evaluate_cv(\n                test_features, X_outer_train, y_outer_train, inner_cv\n            )\n            inner_cv_scores[config_name] = mean_auc\n        \n        # Decision based on INNER CV results\n        # Find best configuration (highest AUC)\n        sorted_configs = sorted(inner_cv_scores.items(), key=lambda x: (-x[1], len(x[0].split('+'))))\n        best_config = sorted_configs[0]\n        \n        # Check if simpler configs are within threshold of best\n        threshold = 0.0003\n        final_config = best_config\n        for config_name, auc in sorted_configs[1:]:\n            if best_config[1] - auc < threshold:\n                if len(config_name.split('+')) < len(best_config[0].split('+')):\n                    final_config = (config_name, auc)\n                    break\n        \n        outer_fold_decisions.append({\n            'fold': outer_fold_idx,\n            'decision': final_config[0],\n            'inner_cv_auc': final_config[1],\n            'all_inner_scores': inner_cv_scores\n        })\n        \n        # Evaluate ALL configurations on OUTER TEST SET (for comparison)\n        for config_name, config_features in configs.items():\n            test_features = baseline_features + config_features\n            model = lgb.LGBMClassifier(**DEFAULT_PARAMS)\n            model.fit(\n                X_outer_train[test_features], y_outer_train,\n                eval_set=[(X_outer_test[test_features], y_outer_test)],\n                eval_metric='logloss',\n                callbacks=[lgb.log_evaluation(0), lgb.early_stopping(50)]\n            )\n            test_auc = roc_auc_score(\n                y_outer_test,\n                model.predict_proba(X_outer_test[test_features])[:, 1]\n            )\n            outer_fold_results[config_name].append(test_auc)\n    \n    # Aggregate results\n    decisions_count = {}\n    for fold_res in outer_fold_decisions:\n        dec = fold_res['decision']\n        decisions_count[dec] = decisions_count.get(dec, 0) + 1\n    \n    # Final decision: majority vote\n    final_recommendation = max(decisions_count.items(), key=lambda x: x[1])[0]\n    \n    # Calculate test performance statistics for each config\n    print(f\"  Test performance across {outer_cv.n_splits} folds:\")\n    config_stats = []\n    for config_name in sorted(configs.keys(), key=lambda x: (-np.mean(outer_fold_results[x]), len(x.split('+')))):\n        mean_auc = np.mean(outer_fold_results[config_name])\n        std_auc = np.std(outer_fold_results[config_name])\n        se_auc = std_auc / np.sqrt(len(outer_fold_results[config_name]))\n        config_stats.append({\n            'config': config_name,\n            'mean': mean_auc,\n            'std': std_auc,\n            'se': se_auc\n        })\n        print(f\"    {config_name:30s}: {format_auc_with_std(mean_auc, std_auc)} (SE: {se_auc:.4f})\")\n    \n    print(f\"  Decision votes: {decisions_count}\")\n    print(f\"  → Final recommendation: {final_recommendation}\")\n    \n    # Store results\n    final_config_stats = [s for s in config_stats if s['config'] == final_recommendation][0]\n    computation_redundancy_results.append({\n        'metric': metric,\n        'recommended_config': final_recommendation,\n        'test_auc_mean': final_config_stats['mean'],\n        'test_auc_std': final_config_stats['std'],\n        'test_auc_se': final_config_stats['se'],\n        'decision_votes': str(decisions_count),\n        'features_to_keep': configs[final_recommendation],\n        'num_features': len(configs[final_recommendation]),\n        'all_config_stats': config_stats\n    })\n\n# Summary table\nprint(\"\\n\" + \"=\" * 90)\nprint(\"EXPERIMENT 2 SUMMARY (NESTED CV)\")\nprint(\"=\" * 90)\n\nsummary_data = []\nfor result in computation_redundancy_results:\n    summary_data.append({\n        'metric': result['metric'],\n        'recommended_config': result['recommended_config'],\n        'test_auc_mean': result['test_auc_mean'],\n        'test_auc_std': result['test_auc_std'],\n        'num_features': result['num_features'],\n        'decision_votes': result['decision_votes']\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df.to_string(index=False))\n\n# Save detailed results\nimport json\nwith open('temporal_computation_redundancy_nested_cv.json', 'w') as f:\n    # Convert to serializable format\n    export_data = []\n    for result in computation_redundancy_results:\n        export_data.append({\n            'metric': result['metric'],\n            'recommended_config': result['recommended_config'],\n            'test_auc_mean': result['test_auc_mean'],\n            'test_auc_std': result['test_auc_std'],\n            'features_to_keep': result['features_to_keep'],\n            'num_features': result['num_features']\n        })\n    json.dump(export_data, f, indent=2)\n\nprint(\"\\n✓ Saved results to: temporal_computation_redundancy_nested_cv.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338677e",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# EXPERIMENT 3: Metric Prioritization (5×3 NESTED CV)\n# Which metrics benefit most from temporal features?\n# Using nested CV for robust evaluation\n# ============================================================================\n\nprint(\"=\" * 90)\nprint(\"EXPERIMENT 3: METRIC PRIORITIZATION (5×3 NESTED CV)\")\nprint(\"=\" * 90)\nprint(\"Testing: Impact of removing each metric's temporal features\")\nprint(\"Method: Ablation study with nested CV\")\nprint(\"-\" * 90)\n\n# Get recommended features from Experiment 2\nrecommended_temporal_features = {}\n\n# For metrics with computation redundancy results, use those\nfor result in computation_redundancy_results:\n    recommended_temporal_features[result['metric']] = result['features_to_keep']\n\n# For metrics with only one computation type, use window selection results\nfor metric, computations in temporal_feature_groups.items():\n    if metric not in recommended_temporal_features:\n        # Use results from window selection if available\n        all_features = []\n        for comp_type, features in computations.items():\n            matching = window_df[\n                (window_df['metric'] == metric) & \n                (window_df['computation'] == comp_type)\n            ]\n            if len(matching) > 0:\n                all_features.extend(matching.iloc[0]['features_to_keep'])\n            else:\n                all_features.extend(features)\n        recommended_temporal_features[metric] = all_features\n\n# Flatten to get all recommended temporal features\nall_recommended_temporal = []\nfor features in recommended_temporal_features.values():\n    all_recommended_temporal.extend(features)\n\nprint(f\"Total recommended temporal features: {len(all_recommended_temporal)}\")\nprint(f\"Reduction from original: {len(all_temporal_features)} → {len(all_recommended_temporal)} ({100*(len(all_temporal_features) - len(all_recommended_temporal))/len(all_temporal_features):.1f}%)\")\n\n# Baseline: all recommended temporal features\nbaseline_with_all_temporal = baseline_features + all_recommended_temporal\n\n# Test impact of each metric using nested CV\nmetric_importance_results = []\n\nprint(\"\\n\" + \"-\" * 90)\nprint(\"Testing impact of removing each metric's temporal features:\")\nprint(\"-\" * 90)\n\nfor metric, features in recommended_temporal_features.items():\n    print(f\"\\n{metric}\")\n    \n    # Features WITHOUT this metric's temporal features\n    features_without_metric = [f for f in baseline_with_all_temporal if f not in features]\n    \n    # Store results across outer folds\n    outer_test_aucs_with = []\n    outer_test_aucs_without = []\n    \n    # OUTER CV LOOP (5 folds)\n    for outer_fold_idx, (outer_train_idx, outer_test_idx) in enumerate(outer_cv.split(X_full, y_full)):\n        X_outer_train = X_full.iloc[outer_train_idx]\n        X_outer_test = X_full.iloc[outer_test_idx]\n        y_outer_train = y_full.iloc[outer_train_idx]\n        y_outer_test = y_full.iloc[outer_test_idx]\n        \n        # Train WITH metric features\n        model_with = lgb.LGBMClassifier(**DEFAULT_PARAMS)\n        model_with.fit(\n            X_outer_train[baseline_with_all_temporal], y_outer_train,\n            eval_set=[(X_outer_test[baseline_with_all_temporal], y_outer_test)],\n            eval_metric='logloss',\n            callbacks=[lgb.log_evaluation(0), lgb.early_stopping(50)]\n        )\n        auc_with = roc_auc_score(\n            y_outer_test,\n            model_with.predict_proba(X_outer_test[baseline_with_all_temporal])[:, 1]\n        )\n        outer_test_aucs_with.append(auc_with)\n        \n        # Train WITHOUT metric features\n        model_without = lgb.LGBMClassifier(**DEFAULT_PARAMS)\n        model_without.fit(\n            X_outer_train[features_without_metric], y_outer_train,\n            eval_set=[(X_outer_test[features_without_metric], y_outer_test)],\n            eval_metric='logloss',\n            callbacks=[lgb.log_evaluation(0), lgb.early_stopping(50)]\n        )\n        auc_without = roc_auc_score(\n            y_outer_test,\n            model_without.predict_proba(X_outer_test[features_without_metric])[:, 1]\n        )\n        outer_test_aucs_without.append(auc_without)\n    \n    # Calculate statistics\n    mean_auc_with = np.mean(outer_test_aucs_with)\n    std_auc_with = np.std(outer_test_aucs_with)\n    mean_auc_without = np.mean(outer_test_aucs_without)\n    std_auc_without = np.std(outer_test_aucs_without)\n    \n    # AUC drop (positive = hurts performance to remove)\n    auc_drops = [w - wo for w, wo in zip(outer_test_aucs_with, outer_test_aucs_without)]\n    mean_drop = np.mean(auc_drops)\n    std_drop = np.std(auc_drops)\n    se_drop = std_drop / np.sqrt(len(auc_drops))\n    \n    # Decision: keep if removing causes significant drop (> 0.0005)\n    keep = mean_drop > 0.0005\n    \n    print(f\"  With metric:    {format_auc_with_std(mean_auc_with, std_auc_with)}\")\n    print(f\"  Without metric: {format_auc_with_std(mean_auc_without, std_auc_without)}\")\n    print(f\"  AUC drop:       {mean_drop:+.6f} ± {std_drop:.4f} (SE: {se_drop:.4f})\")\n    print(f\"  → Decision: {'KEEP' if keep else 'DROP'}\")\n    \n    metric_importance_results.append({\n        'metric': metric,\n        'auc_with_mean': mean_auc_with,\n        'auc_with_std': std_auc_with,\n        'auc_without_mean': mean_auc_without,\n        'auc_without_std': std_auc_without,\n        'auc_drop_mean': mean_drop,\n        'auc_drop_std': std_drop,\n        'auc_drop_se': se_drop,\n        'num_features': len(features),\n        'keep': keep\n    })\n\n# Sort by importance (drop magnitude)\nmetric_df = pd.DataFrame(metric_importance_results).sort_values('auc_drop_mean', ascending=False)\n\nprint(\"\\n\" + \"=\" * 90)\nprint(\"EXPERIMENT 3 SUMMARY - Ranked by Impact (NESTED CV)\")\nprint(\"=\" * 90)\ndisplay_cols = ['metric', 'auc_drop_mean', 'auc_drop_std', 'num_features', 'keep']\nprint(metric_df[display_cols].to_string(index=False))\n\n# Save results\nmetric_df.to_csv('temporal_metric_importance_nested_cv.csv', index=False)\nprint(\"\\n✓ Saved results to: temporal_metric_importance_nested_cv.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb561a",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# FINAL RECOMMENDATIONS: Temporal Feature Selection (NESTED CV)\n# Compile final recommendations and evaluate with unbiased test performance\n# ============================================================================\n\nprint(\"=\" * 90)\nprint(\"FINAL TEMPORAL FEATURE RECOMMENDATIONS (NESTED CV)\")\nprint(\"=\" * 90)\n\n# Compile final feature list based on metric importance\nfinal_temporal_features = []\n\nfor result in metric_importance_results:\n    if result['keep']:\n        final_temporal_features.extend(\n            recommended_temporal_features[result['metric']]\n        )\n\nprint(f\"\\n\" + \"=\" * 70)\nprint(\"TEMPORAL FEATURE REDUCTION SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Original temporal features:      {len(all_temporal_features)}\")\nprint(f\"After window selection:          {len(all_recommended_temporal)}\")\nprint(f\"After metric prioritization:     {len(final_temporal_features)}\")\nprint(f\"Total reduction:                 {len(all_temporal_features) - len(final_temporal_features)} features ({100*(len(all_temporal_features) - len(final_temporal_features))/len(all_temporal_features):.1f}%)\")\n\n# ============================================================================\n# FINAL EVALUATION WITH NESTED CV\n# Compare: Original (all temporal) vs Optimized (selected temporal) vs Baseline (no temporal)\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FINAL MODEL EVALUATION (5-FOLD CV)\")\nprint(\"=\" * 70)\n\n# Define three configurations to test\nconfigs = {\n    'Baseline (no temporal)': baseline_features,\n    'Original (all temporal)': baseline_features + all_temporal_features,\n    'Optimized (selected temporal)': baseline_features + final_temporal_features\n}\n\nprint(f\"Testing {len(configs)} configurations across {outer_cv.n_splits} folds...\")\n\n# Evaluate each configuration using outer CV (no inner CV needed - final evaluation only)\nconfig_results = {}\n\nfor config_name, features in configs.items():\n    print(f\"\\n{config_name}\")\n    print(f\"  Features: {len(features)} ({len([f for f in features if f in all_temporal_features])} temporal)\")\n    \n    test_aucs = []\n    \n    for outer_fold_idx, (outer_train_idx, outer_test_idx) in enumerate(outer_cv.split(X_full, y_full)):\n        X_outer_train = X_full.iloc[outer_train_idx]\n        X_outer_test = X_full.iloc[outer_test_idx]\n        y_outer_train = y_full.iloc[outer_train_idx]\n        y_outer_test = y_full.iloc[outer_test_idx]\n        \n        model = lgb.LGBMClassifier(**DEFAULT_PARAMS)\n        model.fit(\n            X_outer_train[features], y_outer_train,\n            eval_set=[(X_outer_test[features], y_outer_test)],\n            eval_metric='logloss',\n            callbacks=[lgb.log_evaluation(0), lgb.early_stopping(50)]\n        )\n        \n        test_auc = roc_auc_score(\n            y_outer_test,\n            model.predict_proba(X_outer_test[features])[:, 1]\n        )\n        test_aucs.append(test_auc)\n        print(f\"    Fold {outer_fold_idx+1}: {test_auc:.6f}\")\n    \n    mean_auc = np.mean(test_aucs)\n    std_auc = np.std(test_aucs)\n    se_auc = std_auc / np.sqrt(len(test_aucs))\n    \n    print(f\"  → Mean: {format_auc_with_std(mean_auc, std_auc)} (SE: {se_auc:.4f})\")\n    \n    config_results[config_name] = {\n        'mean': mean_auc,\n        'std': std_auc,\n        'se': se_auc,\n        'fold_aucs': test_aucs\n    }\n\n# Statistical comparison\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PERFORMANCE COMPARISON\")\nprint(\"=\" * 70)\n\nbaseline_result = config_results['Baseline (no temporal)']\noriginal_result = config_results['Original (all temporal)']\noptimized_result = config_results['Optimized (selected temporal)']\n\nimprovement_vs_baseline = optimized_result['mean'] - baseline_result['mean']\nimprovement_vs_original = optimized_result['mean'] - original_result['mean']\n\nprint(f\"\\nBaseline (no temporal):         {format_auc_with_std(baseline_result['mean'], baseline_result['std'])}\")\nprint(f\"Original (all temporal):        {format_auc_with_std(original_result['mean'], original_result['std'])}\")\nprint(f\"Optimized (selected temporal):  {format_auc_with_std(optimized_result['mean'], optimized_result['std'])}\")\nprint(f\"\\nImprovement over baseline:      {improvement_vs_baseline:+.6f} ({100*improvement_vs_baseline/baseline_result['mean']:+.2f}%)\")\nprint(f\"Improvement over original:      {improvement_vs_original:+.6f} ({100*improvement_vs_original/original_result['mean']:+.2f}%)\")\nprint(f\"Feature reduction:              {len(all_temporal_features)} → {len(final_temporal_features)} ({100*(len(all_temporal_features) - len(final_temporal_features))/len(all_temporal_features):.1f}%)\")\n\n# Breakdown by metric\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FINAL TEMPORAL FEATURES BY METRIC\")\nprint(\"=\" * 70)\n\nfor metric in sorted(recommended_temporal_features.keys()):\n    if metric in [r['metric'] for r in metric_importance_results if r['keep']]:\n        features = recommended_temporal_features[metric]\n        print(f\"\\n{metric.upper()} ({len(features)} features):\")\n        for f in sorted(features):\n            print(f\"  - {f}\")\n\n# ============================================================================\n# EXPORT RESULTS\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EXPORTING RESULTS\")\nprint(\"=\" * 70)\n\n# Save final temporal feature list\nwith open('final_temporal_features_nested_cv.txt', 'w') as f:\n    f.write(f\"# Final Temporal Features After Systematic Selection (5×3 Nested CV)\\n\")\n    f.write(f\"# Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    f.write(f\"# Original: {len(all_temporal_features)} → Final: {len(final_temporal_features)}\\n\")\n    f.write(f\"# Reduction: {100*(len(all_temporal_features) - len(final_temporal_features))/len(all_temporal_features):.1f}%\\n\")\n    f.write(f\"# Mean test AUC: {optimized_result['mean']:.6f} ± {optimized_result['std']:.4f}\\n\")\n    f.write(f\"# Improvement over baseline: {improvement_vs_baseline:+.6f}\\n\")\n    f.write(f\"# Improvement over original: {improvement_vs_original:+.6f}\\n\\n\")\n    for feat in sorted(final_temporal_features):\n        f.write(feat + '\\n')\n\nprint(\"✓ Saved: final_temporal_features_nested_cv.txt\")\n\n# Save comparison results\ncomparison_df = pd.DataFrame([\n    {\n        'Configuration': name,\n        'Num_Features': len(configs[name]),\n        'Temporal_Features': len([f for f in configs[name] if f in all_temporal_features]),\n        'Mean_AUC': res['mean'],\n        'Std_AUC': res['std'],\n        'SE_AUC': res['se']\n    }\n    for name, res in config_results.items()\n])\ncomparison_df.to_csv('temporal_final_comparison_nested_cv.csv', index=False)\nprint(\"✓ Saved: temporal_final_comparison_nested_cv.csv\")\n\n# Save comprehensive summary\nsummary_nested_cv = {\n    'Methodology': '5×3 Nested Cross-Validation',\n    'Original Temporal Features': len(all_temporal_features),\n    'After Window Selection': len(all_recommended_temporal),\n    'After Metric Prioritization': len(final_temporal_features),\n    'Total Reduction': len(all_temporal_features) - len(final_temporal_features),\n    'Reduction %': f\"{100*(len(all_temporal_features) - len(final_temporal_features))/len(all_temporal_features):.1f}%\",\n    'Baseline Mean AUC': f\"{baseline_result['mean']:.6f}\",\n    'Baseline Std AUC': f\"{baseline_result['std']:.6f}\",\n    'Original Mean AUC': f\"{original_result['mean']:.6f}\",\n    'Original Std AUC': f\"{original_result['std']:.6f}\",\n    'Optimized Mean AUC': f\"{optimized_result['mean']:.6f}\",\n    'Optimized Std AUC': f\"{optimized_result['std']:.6f}\",\n    'Improvement vs Baseline': f\"{improvement_vs_baseline:+.6f}\",\n    'Improvement vs Original': f\"{improvement_vs_original:+.6f}\"\n}\n\nsummary_df = pd.DataFrame([summary_nested_cv]).T\nsummary_df.columns = ['Value']\nsummary_df.to_csv('temporal_feature_selection_summary_nested_cv.csv')\nprint(\"✓ Saved: temporal_feature_selection_summary_nested_cv.csv\")\n\nprint(\"\\n\" + \"=\" * 90)\nprint(\"✓ TEMPORAL FEATURE SELECTION COMPLETE (5×3 NESTED CV)!\")\nprint(\"=\" * 90)\nprint(\"\\nGenerated files:\")\nprint(\"  - temporal_window_selection_nested_cv.csv\")\nprint(\"  - temporal_computation_redundancy_nested_cv.json\")\nprint(\"  - temporal_metric_importance_nested_cv.csv\")\nprint(\"  - final_temporal_features_nested_cv.txt\")\nprint(\"  - temporal_final_comparison_nested_cv.csv\")\nprint(\"  - temporal_feature_selection_summary_nested_cv.csv\")\nprint(\"\\n\" + \"=\" * 90)\nprint(\"METHODOLOGY IMPROVEMENTS:\")\nprint(\"  ✓ Reduced selection bias via inner 3-fold CV averaging\")\nprint(\"  ✓ Unbiased performance estimates via outer 5-fold CV\")\nprint(\"  ✓ Uncertainty quantification (std, SE) for all decisions\")\nprint(\"  ✓ Majority voting across folds for robust feature selection\")\nprint(\"  ✓ Standard error reported: ~\" + f\"{optimized_result['se']:.4f}\" + \" AUC\")\nprint(\"=\" * 90)"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}