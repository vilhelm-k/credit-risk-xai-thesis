{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Optuna\n",
    "\n",
    "This notebook performs hyperparameter optimization for the LightGBM model using Optuna.\n",
    "We optimize all relevant hyperparameters to find the best configuration for credit risk prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from credit_risk_xai.config import FEATURE_CACHE_PATH\n",
    "from credit_risk_xai.features.engineer import prepare_modeling_data\n",
    "from credit_risk_xai.modeling.utils import split_train_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter data (same filters as in 05a notebook)\n",
    "df = pd.read_parquet(FEATURE_CACHE_PATH)\n",
    "df = df[\n",
    "    (df[\"ser_aktiv\"] == 1) & \n",
    "    (df[\"sme_category\"].isin([\"Small\", \"Medium\"])) & \n",
    "    (df[\"knc_kncfall\"] == 1) &\n",
    "    (df[\"bransch_borsbransch_konv\"] != \"40.0\")\n",
    "]\n",
    "\n",
    "X, y = prepare_modeling_data(df)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation (same split as training)\n",
    "X_train, X_val, y_train, y_val = split_train_validation(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Training positive rate: {y_train.mean():.4f}\")\n",
    "print(f\"Validation positive rate: {y_val.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optuna Objective Function\n",
    "\n",
    "We optimize all relevant LightGBM hyperparameters:\n",
    "- **Tree structure**: `num_leaves`, `max_depth`, `min_child_samples`, `min_child_weight`\n",
    "- **Regularization**: `reg_alpha` (L1), `reg_lambda` (L2), `min_split_gain`\n",
    "- **Sampling**: `subsample` (bagging), `colsample_bytree` (feature fraction), `subsample_freq`\n",
    "- **Learning**: `learning_rate`, `n_estimators` (with early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Optuna objective function for LightGBM hyperparameter optimization.\n",
    "    Uses cross-validation for robust evaluation. Minimizes log loss.\n",
    "    \"\"\"\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \n",
    "        # Learning rate and boosting rounds\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"n_estimators\": 10_000,  # Use early stopping\n",
    "        \n",
    "        # Tree structure\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 10.0, log=True),\n",
    "        \n",
    "        # Regularization\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \n",
    "        # Sampling\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 0, 10),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \n",
    "        # Class imbalance handling\n",
    "        \"is_unbalance\": False,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Cross-validation with stratified folds\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_fold_train = X_train.iloc[train_idx]\n",
    "        y_fold_train = y_train.iloc[train_idx]\n",
    "        X_fold_val = X_train.iloc[val_idx]\n",
    "        y_fold_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        # Pruning callback using log loss (aligned with study direction=minimize)\n",
    "        pruning_callback = LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "        \n",
    "        model.fit(\n",
    "            X_fold_train,\n",
    "            y_fold_train,\n",
    "            eval_set=[(X_fold_val, y_fold_val)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                pruning_callback,\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # Evaluate on fold validation set\n",
    "        y_pred_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "        fold_score = log_loss(y_fold_val, y_pred_proba)\n",
    "        cv_scores.append(fold_score)\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",  # Minimize log loss\n",
    "    study_name=\"lightgbm_credit_risk\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=20),\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=100,  # Adjust based on available compute time\n",
    "    show_progress_bar=True,\n",
    "    n_jobs=1,  # Sequential trials (parallelism handled within each trial)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best results\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (Log Loss): {study.best_trial.value:.4f}\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Quick AUC check with best params\n",
    "best_params_check = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"n_estimators\": 10_000,\n",
    "    **study.best_trial.params,\n",
    "}\n",
    "\n",
    "model_check = lgb.LGBMClassifier(**best_params_check)\n",
    "model_check.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)],\n",
    ")\n",
    "\n",
    "y_pred_check = model_check.predict_proba(X_val)[:, 1]\n",
    "print(f\"\\nValidation metrics with best params:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_val, y_pred_check):.4f}\")\n",
    "print(f\"  Log Loss: {log_loss(y_val, y_pred_check):.4f}\")\n",
    "print(f\"  Best iteration: {model_check.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization history\n",
    "import plotly\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter importances\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel coordinate plot\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice plot for key parameters\n",
    "fig = optuna.visualization.plot_slice(\n",
    "    study, \n",
    "    params=[\"learning_rate\", \"num_leaves\", \"max_depth\", \"reg_alpha\", \"reg_lambda\"]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Final Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct best parameters dict\n",
    "best_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"n_estimators\": 10_000,\n",
    "    **study.best_trial.params,\n",
    "}\n",
    "\n",
    "print(\"Best parameters for final model:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training set\n",
    "final_model = lgb.LGBMClassifier(**best_params)\n",
    "\n",
    "final_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "        lgb.log_evaluation(period=100),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {final_model.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out validation set\n",
    "y_pred_proba = final_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "metrics = {\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_pred_proba),\n",
    "    \"PR-AUC\": average_precision_score(y_val, y_pred_proba),\n",
    "    \"Log Loss\": log_loss(y_val, y_pred_proba),\n",
    "    \"Brier Score\": brier_score_loss(y_val, y_pred_proba),\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Model Validation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best parameters for use in other notebooks\n",
    "import json\n",
    "from credit_risk_xai.config import PROJ_ROOT\n",
    "\n",
    "output_path = PROJ_ROOT / \"models\" / \"best_lgbm_params.json\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert to serializable format\n",
    "params_to_save = {k: v for k, v in best_params.items() if k not in [\"verbosity\", \"n_jobs\"]}\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(params_to_save, f, indent=2)\n",
    "\n",
    "print(f\"Best parameters saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print parameters in a format ready to copy into code\n",
    "print(\"\\n# Copy these parameters to use in run_lightgbm_training():\")\n",
    "print(\"params = {\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    if isinstance(v, str):\n",
    "        print(f'    \"{k}\": \"{v}\",')\n",
    "    elif isinstance(v, bool):\n",
    "        print(f'    \"{k}\": {v},')\n",
    "    elif isinstance(v, float):\n",
    "        print(f'    \"{k}\": {v:.6g},')\n",
    "    else:\n",
    "        print(f'    \"{k}\": {v},')\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with default parameters for comparison\n",
    "default_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"n_estimators\": 10_000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbosity\": -1,\n",
    "    \"is_unbalance\": False,\n",
    "    \"metric\": \"binary_logloss\",\n",
    "}\n",
    "\n",
    "default_model = lgb.LGBMClassifier(**default_params)\n",
    "default_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "    ],\n",
    ")\n",
    "\n",
    "y_pred_default = default_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "default_metrics = {\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_pred_default),\n",
    "    \"PR-AUC\": average_precision_score(y_val, y_pred_default),\n",
    "    \"Log Loss\": log_loss(y_val, y_pred_default),\n",
    "    \"Brier Score\": brier_score_loss(y_val, y_pred_default),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    \"Default\": default_metrics,\n",
    "    \"Optimized\": metrics,\n",
    "}).T\n",
    "\n",
    "comparison[\"\u0394 ROC-AUC\"] = comparison[\"ROC-AUC\"] - comparison.loc[\"Default\", \"ROC-AUC\"]\n",
    "comparison[\"\u0394 Log Loss\"] = comparison[\"Log Loss\"] - comparison.loc[\"Default\", \"Log Loss\"]\n",
    "\n",
    "print(\"\\nComparison: Default vs Optimized Parameters\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Optuna optimization searched over:\n",
    "- **Learning rate**: 0.01 - 0.3 (log scale)\n",
    "- **Tree structure**: num_leaves (8-256), max_depth (3-12), min_child_samples (5-100)\n",
    "- **Regularization**: L1 (reg_alpha), L2 (reg_lambda), min_split_gain\n",
    "- **Sampling**: subsample (0.5-1.0), colsample_bytree (0.5-1.0)\n",
    "- **Class imbalance**: is_unbalance or scale_pos_weight\n",
    "\n",
    "The best parameters have been saved to `models/best_lgbm_params.json` for use in other notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}