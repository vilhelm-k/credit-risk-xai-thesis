{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d28576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "\n",
    "PROJ_ROOT = Path.cwd().parent\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJ_ROOT))\n",
    "\n",
    "from credit_risk_xai.modeling.train import DEFAULT_PARAMS\n",
    "\n",
    "from credit_risk_xai.config import FEATURE_CACHE_PATH, FEATURES_FOR_MODEL\n",
    "from credit_risk_xai.features.engineer import prepare_modeling_data\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_calibration_curve(y_true, y_pred_proba, n_bins=100, model_name=\"Model\"):\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_true, y_pred_proba, n_bins=n_bins, strategy='quantile'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=model_name)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Calibration Curve - {model_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ECE (Expected Calibration Error)\n",
    "    ece = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    print(f\"ECE: {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter data\n",
    "df = pd.read_parquet(FEATURE_CACHE_PATH)\n",
    "df = df[\n",
    "    (df[\"ser_aktiv\"] == 1) & \n",
    "    (df[\"sme_category\"].isin([\"Small\", \"Medium\"])) & \n",
    "    (df[\"knc_kncfall\"] == 1) &\n",
    "    (df[\"bransch_borsbransch_konv\"] != \"40.0\")\n",
    "    ]\n",
    "X, y = prepare_modeling_data(df)\n",
    "\n",
    "print(f\"Features: {X.shape[1]} | Samples: {len(X):,}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "print(f\"Imbalance: {(y==0).sum()/(y==1).sum():.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit_risk_xai.modeling.train import run_lightgbm_training\n",
    "\n",
    "results = run_lightgbm_training(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    dataset_description=\"ser_aktiv==1 & SME\u2208{Small,Medium} & knc_kncfall==1\",  # optional note for W&B\n",
    "    use_wandb=False,\n",
    "    wandb_project=\"credit-risk-xai\",\n",
    "    wandb_run_name=\"lgbm_finalish_prune\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model = results[\"model\"]\n",
    "X_train = results[\"X_train\"]\n",
    "X_val = results[\"X_val\"]\n",
    "y_train = results[\"y_train\"]\n",
    "y_val = results[\"y_val\"]\n",
    "y_pred_proba = results[\"y_val_proba\"]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_val, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_val, y_pred_proba)\n",
    "\n",
    "print(f\"LightGBM Performance:\")\n",
    "print(f\"  AUC: {auc:.4f}\")\n",
    "print(f\"  PR-AUC: {pr_auc:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_pred, y_val))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "plot_calibration_curve(y_val, y_pred_proba, model_name=\"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jl4wv9eogzf",
   "metadata": {},
   "source": [
    "# Phase 1: Foundation & Baseline Comparison\n",
    "\n",
    "Following the comprehensive XAI thesis plan, we'll establish baselines and infrastructure for all subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603eol3bjth",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Train Logistic Regression Baseline with Winsorization + Target Encoding\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "print(\"Training Logistic Regression with Winsorization + Target Encoding...\")\n",
    "print(f\"Training set: {len(X_train):,} samples | Validation set: {len(X_val):,} samples\")\n",
    "print(f\"Training imbalance: {(y_train==0).sum()/(y_train==1).sum():.1f}:1\")\n",
    "\n",
    "# Winsorization function for continuous features\n",
    "def winsorize_features(X, limits=(0.01, 0.01), categorical_features=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Winsorize continuous features at specified percentiles.\n",
    "    \n",
    "    Categorical features are excluded from winsorization.\n",
    "    Extreme values beyond 1st/99th percentile are capped to those thresholds.\n",
    "    This prevents coefficient instability in logistic regression while preserving\n",
    "    the signal that \"extreme values indicate higher risk\".\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Features to winsorize\n",
    "    limits : tuple\n",
    "        (lower, upper) proportions to winsorize (default: 1st and 99th percentile)\n",
    "    categorical_features : list\n",
    "        List of categorical feature names to exclude from winsorization\n",
    "    verbose : bool\n",
    "        Print progress messages\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Winsorized features\n",
    "    \"\"\"\n",
    "    X_wins = X.copy()\n",
    "    \n",
    "    if categorical_features is None:\n",
    "        categorical_features = []\n",
    "    \n",
    "    # Identify categorical columns (category dtype)\n",
    "    cat_cols = X.select_dtypes(include=['category']).columns.tolist()\n",
    "    categorical_features = list(set(categorical_features + cat_cols))\n",
    "    \n",
    "    continuous_cols = [col for col in X.columns if col not in categorical_features]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Winsorizing {len(continuous_cols)} continuous features at {limits[0]*100:.0f}th/{(1-limits[1])*100:.0f}th percentiles\")\n",
    "        print(f\"  Excluding {len(categorical_features)} categorical features: {categorical_features}\")\n",
    "    \n",
    "    for col in continuous_cols:\n",
    "        # Winsorize only non-NaN values\n",
    "        mask = X_wins[col].notna()\n",
    "        if mask.sum() > 0:\n",
    "            # Convert to numpy array to handle pandas nullable dtypes (Int16, etc.)\n",
    "            values = X_wins.loc[mask, col].values.astype(float)\n",
    "            winsorized_values = winsorize(values, limits=limits)\n",
    "            X_wins.loc[mask, col] = winsorized_values\n",
    "    \n",
    "    return X_wins\n",
    "\n",
    "def target_encode_categorical(X_train, X_val, y_train, categorical_cols, smoothing=10.0):\n",
    "    \"\"\"\n",
    "    Target encoding for categorical variables using median (smoothed mean) of target.\n",
    "    \n",
    "    This is a form of \"fixed effects\" encoding where each category gets replaced\n",
    "    by its empirical default rate, smoothed toward the global mean to handle\n",
    "    rare categories.\n",
    "    \n",
    "    Formula: encoded_value = (n_cat * mean_cat + smoothing * global_mean) / (n_cat + smoothing)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    X_val : pd.DataFrame\n",
    "        Validation features  \n",
    "    y_train : pd.Series\n",
    "        Training target\n",
    "    categorical_cols : list\n",
    "        Categorical column names\n",
    "    smoothing : float\n",
    "        Smoothing factor (higher = more regularization toward global mean)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_train_encoded, X_val_encoded, encoding_maps)\n",
    "    \"\"\"\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_val_enc = X_val.copy()\n",
    "    \n",
    "    global_mean = y_train.mean()\n",
    "    encoding_maps = {}\n",
    "    \n",
    "    print(f\"\\n  Target encoding {len(categorical_cols)} categorical features:\")\n",
    "    print(f\"  Global default rate: {global_mean:.4f}\")\n",
    "    print(f\"  Smoothing parameter: {smoothing}\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col not in X_train.columns:\n",
    "            continue\n",
    "            \n",
    "        # Compute smoothed mean for each category\n",
    "        category_stats = pd.DataFrame({\n",
    "            'count': X_train.groupby(col, observed=True).size(),\n",
    "            'sum': y_train.groupby(X_train[col], observed=True).sum()\n",
    "        })\n",
    "        \n",
    "        category_stats['smoothed_mean'] = (\n",
    "            (category_stats['sum'] + smoothing * global_mean) / \n",
    "            (category_stats['count'] + smoothing)\n",
    "        )\n",
    "        \n",
    "        encoding_map = category_stats['smoothed_mean'].to_dict()\n",
    "        encoding_maps[col] = encoding_map\n",
    "        \n",
    "        # Apply encoding\n",
    "        X_train_enc[col] = X_train[col].map(encoding_map).fillna(global_mean)\n",
    "        X_val_enc[col] = X_val[col].map(encoding_map).fillna(global_mean)\n",
    "        \n",
    "        n_categories = len(encoding_map)\n",
    "        min_enc = category_stats['smoothed_mean'].min()\n",
    "        max_enc = category_stats['smoothed_mean'].max()\n",
    "        \n",
    "        print(f\"    {col}: {n_categories} categories \u2192 range [{min_enc:.4f}, {max_enc:.4f}]\")\n",
    "    \n",
    "    return X_train_enc, X_val_enc, encoding_maps\n",
    "\n",
    "# Step 1: Identify categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Step 2: Target encode categoricals BEFORE winsorization\n",
    "# (Target encoding creates continuous values from categories)\n",
    "X_train_enc, X_val_enc, encoding_maps = target_encode_categorical(\n",
    "    X_train, X_val, y_train, \n",
    "    categorical_cols=categorical_cols,\n",
    "    smoothing=10.0  # Regularization: higher = more conservative\n",
    ")\n",
    "\n",
    "# Step 3: Apply winsorization to all features (now all continuous)\n",
    "X_train_wins = winsorize_features(X_train_enc, limits=(0.01, 0.01), verbose=True)\n",
    "X_val_wins = winsorize_features(X_val_enc, limits=(0.01, 0.01), verbose=False)\n",
    "\n",
    "# Step 4: Create preprocessing pipeline (median imputation + scaling)\n",
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit preprocessor on winsorized training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train_wins)\n",
    "X_val_processed = preprocessor.transform(X_val_wins)\n",
    "\n",
    "print(f\"\\nPreprocessing complete:\")\n",
    "print(f\"  1. Target encoding: Categorical \u2192 smoothed default rates\")\n",
    "print(f\"  2. Winsorization: Capped extreme values at 1st/99th percentile\")\n",
    "print(f\"  3. Imputation: Median imputation for missing values\")\n",
    "print(f\"  4. Scaling: StandardScaler (mean=0, std=1)\")\n",
    "\n",
    "# Train logistic regression with L2 regularization\n",
    "logit_model = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    class_weight=None,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Logistic Regression (C=1.0, L2 penalty)...\")\n",
    "logit_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predictions\n",
    "logit_train_proba = logit_model.predict_proba(X_train_processed)[:, 1]\n",
    "logit_val_proba = logit_model.predict_proba(X_val_processed)[:, 1]\n",
    "logit_val_pred = (logit_val_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"\u2713 Logistic Regression trained with target-encoded categoricals + winsorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k2klxkfxjc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 Model Performance Comparison\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "def compute_ece(y_true, y_pred_proba, n_bins=100):\n",
    "    \"\"\"Compute Expected Calibration Error\"\"\"\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_true, y_pred_proba, n_bins=n_bins, strategy='quantile'\n",
    "    )\n",
    "    ece = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    return ece\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"=\" * 80)\n",
    "print(\"TABLE 1: MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metrics = {\n",
    "    'Model': ['LightGBM', 'Logistic Regression', '\u0394 (LightGBM - Logit)'],\n",
    "    'AUC': [],\n",
    "    'PR-AUC': [],\n",
    "    'Brier Score': [],\n",
    "    'ECE': []\n",
    "}\n",
    "\n",
    "# LightGBM metrics\n",
    "lgbm_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "lgbm_pr_auc = average_precision_score(y_val, y_pred_proba)\n",
    "lgbm_brier = brier_score_loss(y_val, y_pred_proba)\n",
    "lgbm_ece = compute_ece(y_val, y_pred_proba)\n",
    "\n",
    "metrics['AUC'].append(lgbm_auc)\n",
    "metrics['PR-AUC'].append(lgbm_pr_auc)\n",
    "metrics['Brier Score'].append(lgbm_brier)\n",
    "metrics['ECE'].append(lgbm_ece)\n",
    "\n",
    "# Logistic Regression metrics\n",
    "logit_auc = roc_auc_score(y_val, logit_val_proba)\n",
    "logit_pr_auc = average_precision_score(y_val, logit_val_proba)\n",
    "logit_brier = brier_score_loss(y_val, logit_val_proba)\n",
    "logit_ece = compute_ece(y_val, logit_val_proba)\n",
    "\n",
    "metrics['AUC'].append(logit_auc)\n",
    "metrics['PR-AUC'].append(logit_pr_auc)\n",
    "metrics['Brier Score'].append(logit_brier)\n",
    "metrics['ECE'].append(logit_ece)\n",
    "\n",
    "# Differences\n",
    "metrics['AUC'].append(lgbm_auc - logit_auc)\n",
    "metrics['PR-AUC'].append(lgbm_pr_auc - logit_pr_auc)\n",
    "metrics['Brier Score'].append(lgbm_brier - logit_brier)\n",
    "metrics['ECE'].append(lgbm_ece - logit_ece)\n",
    "\n",
    "performance_df = pd.DataFrame(metrics)\n",
    "print(performance_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNotes:\")\n",
    "print(\"  - AUC, PR-AUC: Higher is better\")\n",
    "print(\"  - Brier Score, ECE: Lower is better (better calibration)\")\n",
    "print(f\"  - LightGBM improves AUC by {(lgbm_auc - logit_auc)*100:.2f} percentage points\")\n",
    "print(f\"  - LightGBM improves PR-AUC by {(lgbm_pr_auc - logit_pr_auc)*100:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mnaa5qr2is",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calibration Curves Comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# LightGBM calibration\n",
    "from sklearn.calibration import calibration_curve\n",
    "fraction_pos_lgbm, mean_pred_lgbm = calibration_curve(\n",
    "    y_val, y_pred_proba, n_bins=100, strategy='quantile'\n",
    ")\n",
    "axes[0].plot(mean_pred_lgbm, fraction_pos_lgbm, 's-', label='LightGBM', linewidth=2)\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect calibration', alpha=0.5)\n",
    "axes[0].set_xlabel('Mean predicted probability', fontsize=12)\n",
    "axes[0].set_ylabel('Fraction of positives', fontsize=12)\n",
    "axes[0].set_title(f'LightGBM Calibration (ECE={lgbm_ece:.4f})', fontsize=13)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Logistic Regression calibration\n",
    "fraction_pos_logit, mean_pred_logit = calibration_curve(\n",
    "    y_val, logit_val_proba, n_bins=100, strategy='quantile'\n",
    ")\n",
    "axes[1].plot(mean_pred_logit, fraction_pos_logit, 's-', label='Logistic Regression', \n",
    "             linewidth=2, color='orange')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect calibration', alpha=0.5)\n",
    "axes[1].set_xlabel('Mean predicted probability', fontsize=12)\n",
    "axes[1].set_ylabel('Fraction of positives', fontsize=12)\n",
    "axes[1].set_title(f'Logistic Regression Calibration (ECE={logit_ece:.4f})', fontsize=13)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCalibration Analysis:\")\n",
    "print(f\"  LightGBM ECE: {lgbm_ece:.4f} - {'Well calibrated' if lgbm_ece < 0.05 else 'Needs calibration'}\")\n",
    "print(f\"  Logistic ECE: {logit_ece:.4f} - {'Well calibrated' if logit_ece < 0.05 else 'Needs calibration'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w45p0b5d9pq",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.3 Compute SHAP Values for Both Models\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Computing SHAP values...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# LightGBM SHAP values\n",
    "print(\"\\n[1/2] Computing SHAP for LightGBM...\")\n",
    "print(f\"  Sample size: {len(X_val):,} observations\")\n",
    "\n",
    "start_time = time.time()\n",
    "explainer_lgbm = shap.TreeExplainer(model)\n",
    "shap_values_lgbm = explainer_lgbm.shap_values(X_val)\n",
    "\n",
    "# TreeExplainer returns [neg_class, pos_class] for binary classification\n",
    "# We want SHAP values for the positive class (default/credit event)\n",
    "if isinstance(shap_values_lgbm, list):\n",
    "    shap_values_lgbm = shap_values_lgbm[1]  # Positive class\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"  \u2713 LightGBM SHAP computed in {elapsed:.1f}s\")\n",
    "print(f\"  Shape: {shap_values_lgbm.shape}\")\n",
    "\n",
    "# Logistic Regression SHAP values\n",
    "print(\"\\n[2/2] Computing SHAP for Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# For linear models, use LinearExplainer (much faster than KernelExplainer)\n",
    "# LinearExplainer requires preprocessed data\n",
    "explainer_logit = shap.LinearExplainer(\n",
    "    logit_model, \n",
    "    X_train_processed,  # Background data (preprocessed)\n",
    "    feature_perturbation=\"interventional\"\n",
    ")\n",
    "shap_values_logit = explainer_logit.shap_values(X_val_processed)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"  \u2713 Logistic SHAP computed in {elapsed:.1f}s\")\n",
    "print(f\"  Shape: {shap_values_logit.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SHAP computation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fkwngyphi5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cache SHAP values for reuse\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = PROJ_ROOT / \"results\" / \"xai_exploration\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save SHAP values and related objects\n",
    "cache_file = results_dir / \"shap_cache.pkl\"\n",
    "\n",
    "shap_cache = {\n",
    "    'shap_values_lgbm': shap_values_lgbm,\n",
    "    'shap_values_logit': shap_values_logit,\n",
    "    'explainer_lgbm': explainer_lgbm,\n",
    "    'explainer_logit': explainer_logit,\n",
    "    'X_val': X_val,\n",
    "    'X_val_processed': X_val_processed,\n",
    "    'y_val': y_val,\n",
    "    'y_pred_proba_lgbm': y_pred_proba,\n",
    "    'y_pred_proba_logit': logit_val_proba,\n",
    "    'feature_names': X_val.columns.tolist()\n",
    "}\n",
    "\n",
    "print(f\"Caching SHAP values to: {cache_file}\")\n",
    "with open(cache_file, 'wb') as f:\n",
    "    pickle.dump(shap_cache, f)\n",
    "\n",
    "print(f\"\u2713 SHAP cache saved ({cache_file.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "print(f\"\\nCached objects:\")\n",
    "for key, val in shap_cache.items():\n",
    "    if hasattr(val, 'shape'):\n",
    "        print(f\"  - {key}: {val.shape}\")\n",
    "    elif isinstance(val, list):\n",
    "        print(f\"  - {key}: {len(val)} items\")\n",
    "    else:\n",
    "        print(f\"  - {key}: {type(val).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cl91hs9z",
   "metadata": {},
   "source": [
    "# Phase 2: Global Model Understanding (Chapter 1)\n",
    "\n",
    "In this section, we analyze what the models learned and how they differ from traditional credit theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aayfja9oca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1 Feature Importance Hierarchy\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TABLE 2: FEATURE IMPORTANCE COMPARISON (Top 20 Features)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# SHAP importance for both models (mean absolute SHAP)\n",
    "feature_names = X_val.columns.tolist()\n",
    "\n",
    "lgbm_importance_shap = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'lgbm_shap': np.abs(shap_values_lgbm).mean(axis=0)\n",
    "})\n",
    "\n",
    "logit_importance_shap = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'logit_shap': np.abs(shap_values_logit).mean(axis=0)\n",
    "})\n",
    "\n",
    "# LightGBM gain importance\n",
    "# Note: Use feature_name_ (attribute) or booster_.feature_name() depending on sklearn wrapper\n",
    "lgbm_importance_gain = pd.DataFrame({\n",
    "    'feature': feature_names,  # Use the same feature names from X_val\n",
    "    'lgbm_gain': model.feature_importances_  # This is the sklearn-compatible way\n",
    "})\n",
    "\n",
    "# Logistic regression: use absolute value of standardized coefficients\n",
    "# Standardized coefficients represent importance when features are on same scale\n",
    "logit_coefficients = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'logit_coef': np.abs(logit_model.coef_[0])\n",
    "})\n",
    "\n",
    "# Merge all importance measures\n",
    "importance_comparison = (\n",
    "    lgbm_importance_shap\n",
    "    .merge(logit_importance_shap, on='feature')\n",
    "    .merge(lgbm_importance_gain, on='feature')\n",
    "    .merge(logit_coefficients, on='feature')\n",
    ")\n",
    "\n",
    "# Normalize to 0-100 scale for comparison\n",
    "for col in ['lgbm_shap', 'logit_shap', 'lgbm_gain', 'logit_coef']:\n",
    "    importance_comparison[f'{col}_norm'] = (\n",
    "        100 * importance_comparison[col] / importance_comparison[col].max()\n",
    "    )\n",
    "\n",
    "# Add rankings\n",
    "importance_comparison['lgbm_shap_rank'] = importance_comparison['lgbm_shap'].rank(ascending=False)\n",
    "importance_comparison['logit_shap_rank'] = importance_comparison['logit_shap'].rank(ascending=False)\n",
    "\n",
    "# Sort by LightGBM SHAP importance\n",
    "importance_comparison = importance_comparison.sort_values('lgbm_shap', ascending=False)\n",
    "\n",
    "# Display top 20\n",
    "top_20 = importance_comparison.head(20)[\n",
    "    ['feature', 'lgbm_shap_rank', 'logit_shap_rank', \n",
    "     'lgbm_shap_norm', 'logit_shap_norm', 'lgbm_gain_norm', 'logit_coef_norm']\n",
    "].copy()\n",
    "\n",
    "top_20['rank_diff'] = top_20['logit_shap_rank'] - top_20['lgbm_shap_rank']\n",
    "\n",
    "print(top_20.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNotes:\")\n",
    "print(\"  - *_norm: Normalized importance (0-100 scale)\")\n",
    "print(\"  - rank_diff: Positive = more important in Logit vs LightGBM\")\n",
    "print(\"  - SHAP values measure actual predictive contribution\")\n",
    "print(\"  - Gain measures average information gain from splits (LightGBM only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mkfeh40lcib",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Importance Comparison Visualization\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "# Get top 15 features by LightGBM SHAP\n",
    "top_15_lgbm = importance_comparison.head(15).copy()\n",
    "\n",
    "# Sort by lgbm_shap for plotting (ascending for horizontal bar chart)\n",
    "top_15_lgbm = top_15_lgbm.sort_values('lgbm_shap')\n",
    "\n",
    "y_pos = np.arange(len(top_15_lgbm))\n",
    "\n",
    "# Plot LightGBM SHAP as bars\n",
    "ax.barh(y_pos, top_15_lgbm['lgbm_shap'], \n",
    "        color='steelblue', alpha=0.7, label='LightGBM SHAP', height=0.6)\n",
    "\n",
    "# Overlay Logit SHAP as markers\n",
    "ax.scatter(top_15_lgbm['logit_shap'], y_pos, \n",
    "           color='orange', s=100, marker='D', \n",
    "           label='Logit SHAP', zorder=3, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "# Formatting\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_15_lgbm['feature'], fontsize=11)\n",
    "ax.set_xlabel('Mean |SHAP value|', fontsize=13)\n",
    "ax.set_title('Feature Importance Comparison: LightGBM vs Logistic Regression\\n' + \n",
    "             '(Bars = LightGBM, Diamonds = Logistic)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add vertical line at x=0 for reference\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Add rank_diff column for analysis\n",
    "importance_comparison['rank_diff'] = (\n",
    "    importance_comparison['logit_shap_rank'] - importance_comparison['lgbm_shap_rank']\n",
    ")\n",
    "\n",
    "# Features where LightGBM ranks much higher than Logit\n",
    "lgbm_prioritized = importance_comparison[\n",
    "    (importance_comparison['lgbm_shap_rank'] <= 10) & \n",
    "    (importance_comparison['rank_diff'] > 5)\n",
    "]\n",
    "\n",
    "if len(lgbm_prioritized) > 0:\n",
    "    print(f\"\\n\u2713 Features LightGBM prioritizes over Logit:\")\n",
    "    for _, row in lgbm_prioritized.iterrows():\n",
    "        print(f\"   \u2022 {row['feature']}: LightGBM rank {int(row['lgbm_shap_rank'])}, \"\n",
    "              f\"Logit rank {int(row['logit_shap_rank'])}\")\n",
    "\n",
    "# Features where Logit ranks much higher than LightGBM\n",
    "logit_prioritized = importance_comparison[\n",
    "    (importance_comparison['logit_shap_rank'] <= 10) & \n",
    "    (importance_comparison['rank_diff'] < -5)\n",
    "]\n",
    "\n",
    "if len(logit_prioritized) > 0:\n",
    "    print(f\"\\n\u2713 Features Logit prioritizes over LightGBM:\")\n",
    "    for _, row in logit_prioritized.iterrows():\n",
    "        print(f\"   \u2022 {row['feature']}: Logit rank {int(row['logit_shap_rank'])}, \"\n",
    "              f\"LightGBM rank {int(row['lgbm_shap_rank'])}\")\n",
    "\n",
    "# Summary of differences\n",
    "print(f\"\\n\u2713 Biggest discrepancies:\")\n",
    "top_discrepancies = importance_comparison.nlargest(5, 'rank_diff')[\n",
    "    ['feature', 'lgbm_shap_rank', 'logit_shap_rank', 'rank_diff']\n",
    "]\n",
    "print(\"\\nFeatures MORE important in Logit:\")\n",
    "for _, row in top_discrepancies.iterrows():\n",
    "    if row['rank_diff'] > 0:\n",
    "        print(f\"   \u2022 {row['feature']}: Logit values it {int(row['rank_diff'])} ranks higher\")\n",
    "\n",
    "bottom_discrepancies = importance_comparison.nsmallest(5, 'rank_diff')[\n",
    "    ['feature', 'lgbm_shap_rank', 'logit_shap_rank', 'rank_diff']\n",
    "]\n",
    "print(\"\\nFeatures MORE important in LightGBM:\")\n",
    "for _, row in bottom_discrepancies.iterrows():\n",
    "    if row['rank_diff'] < 0:\n",
    "        print(f\"   \u2022 {row['feature']}: LightGBM values it {int(abs(row['rank_diff']))} ranks higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ki80fkosv",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2 ALE (Accumulated Local Effects) Plots with Model Comparison\n",
    "\n",
    "# Custom ALE implementation\n",
    "def compute_ale_1d(model, X, feature, grid_size=50, predict_fn=None, percentile_range=(5, 95)):\n",
    "    \"\"\"\n",
    "    Compute 1D ALE plot for a given feature.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained model with predict_proba method\n",
    "    X : pd.DataFrame, input features\n",
    "    feature : str, feature name to compute ALE for\n",
    "    grid_size : int, number of grid points (ignored for binary)\n",
    "    predict_fn : callable, custom prediction function (default: model.predict_proba)\n",
    "    percentile_range : tuple, (lower, upper) percentiles to focus ALE on (default: 5th-95th)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (grid_centers, ale_cumsum, counts, percentile_bounds)\n",
    "    \"\"\"\n",
    "    if predict_fn is None:\n",
    "        predict_fn = lambda x: model.predict_proba(x)[:, 1]\n",
    "    \n",
    "    X_work = X.copy()\n",
    "    feat_values = X_work[feature].dropna().values\n",
    "    \n",
    "    # Check if binary/low cardinality\n",
    "    unique_vals = np.unique(feat_values)\n",
    "    is_binary = len(unique_vals) <= 3\n",
    "    \n",
    "    if is_binary:\n",
    "        # Binary/categorical feature: compute ALE for each unique value\n",
    "        print(f\"    Binary/categorical feature with {len(unique_vals)} unique values\")\n",
    "        \n",
    "        # Sort unique values\n",
    "        unique_vals = np.sort(unique_vals)\n",
    "        grid = unique_vals\n",
    "        \n",
    "        # For binary, we just compute the difference between categories\n",
    "        ale_values = []\n",
    "        counts_list = []\n",
    "        \n",
    "        for i in range(len(grid)):\n",
    "            # Count observations at this value\n",
    "            at_value = (X_work[feature] == grid[i]).sum()\n",
    "            counts_list.append(at_value)\n",
    "            \n",
    "            if i == 0:\n",
    "                # First value: baseline (set to 0)\n",
    "                ale_values.append(0)\n",
    "            else:\n",
    "                # Compute difference from previous value\n",
    "                X_sample = X_work.copy()\n",
    "                \n",
    "                # Predict at current value\n",
    "                X_curr = X_sample.copy()\n",
    "                X_curr[feature] = grid[i]\n",
    "                pred_curr = predict_fn(X_curr).mean()\n",
    "                \n",
    "                # Predict at previous value\n",
    "                X_prev = X_sample.copy()\n",
    "                X_prev[feature] = grid[i-1]\n",
    "                pred_prev = predict_fn(X_prev).mean()\n",
    "                \n",
    "                # ALE is cumulative difference\n",
    "                ale_values.append(pred_curr - pred_prev)\n",
    "        \n",
    "        ale_cumsum = np.cumsum(ale_values)\n",
    "        counts = np.array(counts_list)\n",
    "        \n",
    "        # Center at weighted mean\n",
    "        if counts.sum() > 0:\n",
    "            weighted_mean = np.average(ale_cumsum, weights=counts)\n",
    "            ale_cumsum = ale_cumsum - weighted_mean\n",
    "        \n",
    "        grid_centers = grid  # For binary, grid centers are just the values\n",
    "        percentile_bounds = (grid.min(), grid.max())\n",
    "        \n",
    "    else:\n",
    "        # Continuous feature: simple quantile-based grid\n",
    "        p_lower, p_upper = percentile_range\n",
    "        lower_bound = np.percentile(feat_values, p_lower)\n",
    "        upper_bound = np.percentile(feat_values, p_upper)\n",
    "\n",
    "        # Filter to interpretable range (5th-95th percentile)\n",
    "        interpretable_values = feat_values[(feat_values >= lower_bound) & (feat_values <= upper_bound)]\n",
    "\n",
    "        # Create quantile-based grid\n",
    "        quantiles = np.linspace(0, 1, grid_size + 1)\n",
    "        grid = np.quantile(interpretable_values, quantiles)\n",
    "        grid = np.unique(grid)\n",
    "\n",
    "        # Initialize ALE\n",
    "        ale_values = np.zeros(len(grid) - 1)\n",
    "        counts = np.zeros(len(grid) - 1)\n",
    "\n",
    "        # Compute local effects for each interval\n",
    "        for i in range(len(grid) - 1):\n",
    "            in_interval = (X_work[feature] >= grid[i]) & (X_work[feature] < grid[i + 1])\n",
    "\n",
    "            if in_interval.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            X_interval = X_work[in_interval].copy()\n",
    "\n",
    "            # Predict at lower bound\n",
    "            X_lower = X_interval.copy()\n",
    "            X_lower[feature] = grid[i]\n",
    "            pred_lower = predict_fn(X_lower)\n",
    "\n",
    "            # Predict at upper bound\n",
    "            X_upper = X_interval.copy()\n",
    "            X_upper[feature] = grid[i + 1]\n",
    "            pred_upper = predict_fn(X_upper)\n",
    "\n",
    "            # Local effect\n",
    "            ale_values[i] = (pred_upper - pred_lower).mean()\n",
    "            counts[i] = in_interval.sum()\n",
    "\n",
    "        # Accumulate effects\n",
    "        ale_cumsum = np.cumsum(ale_values)\n",
    "\n",
    "        # Center ALE (mean = 0)\n",
    "        valid_counts = counts[counts > 0]\n",
    "        valid_ale = ale_cumsum[counts > 0]\n",
    "\n",
    "        if len(valid_counts) > 0:\n",
    "            ale_cumsum = ale_cumsum - np.average(valid_ale, weights=valid_counts)\n",
    "        else:\n",
    "            ale_cumsum = np.zeros_like(ale_cumsum)\n",
    "\n",
    "        # Grid centers for plotting\n",
    "        grid_centers = (grid[:-1] + grid[1:]) / 2\n",
    "        percentile_bounds = (lower_bound, upper_bound)\n",
    "    \n",
    "    return grid_centers, ale_cumsum, counts, percentile_bounds\n",
    "\n",
    "print(\"\u2713 ALE computation function defined\")\n",
    "print(\"  - Handles binary and continuous features\")\n",
    "print(\"  - Uses simple quantile-based grid (5th-95th percentile)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4boesmdcjo",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate ALE plots for Top 15 CONTINUOUS features (5th-95th percentile range)\n",
    "\n",
    "# Get top continuous features by LightGBM SHAP importance (skip categoricals, get exactly 15)\n",
    "top_features_all = importance_comparison['feature'].tolist()\n",
    "top_continuous_features = []\n",
    "for f in top_features_all:\n",
    "    if X_val[f].dtype.name != 'category':\n",
    "        top_continuous_features.append(f)\n",
    "        if len(top_continuous_features) == 15:\n",
    "            break\n",
    "\n",
    "print(f\"Selected first 15 continuous features (skipped categoricals):\")\n",
    "for i, f in enumerate(top_continuous_features, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "\n",
    "# Prediction functions for both models\n",
    "def predict_lgbm(X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "def predict_logit(X):\n",
    "    \"\"\"\n",
    "    Apply full preprocessing pipeline for logit (SILENT mode).\n",
    "    1. Target encode categoricals\n",
    "    2. Winsorize continuous features  \n",
    "    3. Impute + scale\n",
    "    \"\"\"\n",
    "    # Identify categorical columns\n",
    "    cat_cols = X.select_dtypes(include=['category']).columns.tolist()\n",
    "    \n",
    "    # Target encode using training encodings\n",
    "    X_enc = X.copy()\n",
    "    for col in cat_cols:\n",
    "        if col in encoding_maps:\n",
    "            global_mean = y_train.mean()\n",
    "            X_enc[col] = X[col].map(encoding_maps[col]).fillna(global_mean)\n",
    "    \n",
    "    # Winsorize (SILENT - verbose=False)\n",
    "    X_wins = winsorize_features(X_enc, limits=(0.01, 0.01), verbose=False)\n",
    "    \n",
    "    # Preprocess and predict\n",
    "    X_processed = preprocessor.transform(X_wins)\n",
    "    return logit_model.predict_proba(X_processed)[:, 1]\n",
    "\n",
    "# Compute ALE for continuous features only\n",
    "ale_results = {}\n",
    "\n",
    "for i, feature in enumerate(top_continuous_features, 1):\n",
    "    print(f\"[{i}/15] Computing ALE for {feature}...\")\n",
    "    \n",
    "    try:\n",
    "        # LightGBM ALE (5th-95th percentile)\n",
    "        grid_lgbm, ale_lgbm, counts, bounds = compute_ale_1d(\n",
    "            model, X_val, feature, grid_size=40, predict_fn=predict_lgbm,\n",
    "            percentile_range=(5, 95)\n",
    "        )\n",
    "        \n",
    "        # Logistic Regression ALE (5th-95th percentile)\n",
    "        grid_logit, ale_logit, _, _ = compute_ale_1d(\n",
    "            logit_model, X_val, feature, grid_size=40, predict_fn=predict_logit,\n",
    "            percentile_range=(5, 95)\n",
    "        )\n",
    "        \n",
    "        ale_results[feature] = {\n",
    "            'grid_lgbm': grid_lgbm,\n",
    "            'ale_lgbm': ale_lgbm,\n",
    "            'grid_logit': grid_logit,\n",
    "            'ale_logit': ale_logit,\n",
    "            'counts': counts,\n",
    "            'bounds': bounds,  # (5th percentile, 95th percentile)\n",
    "            'feature_values': X_val[feature].dropna().values\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   \u26a0 Error computing ALE for {feature}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\u2713 ALE computation complete for {len(ale_results)} continuous features\")\n",
    "print(f\"\\nNote:\")\n",
    "print(f\"  \u2022 ALE plots show 5th-95th percentile range (90% of data)\")\n",
    "print(f\"  \u2022 Extreme values beyond this range excluded for interpretability\")\n",
    "print(f\"  \u2022 Categorical features (like sni_group_3digit) shown via SHAP dependence plots instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6wsrmy6b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize ALE Plots\n",
    "\n",
    "n_plots = len(ale_results)\n",
    "n_cols = 5\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (feature, data) in enumerate(ale_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if len(data['grid_lgbm']) == 0:\n",
    "        ax.text(0.5, 0.5, f\"{feature}\\n(no variation)\", \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(feature, fontsize=9, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    is_binary = len(data['grid_lgbm']) <= 3\n",
    "    \n",
    "    ax.plot(data['grid_lgbm'], data['ale_lgbm'], \n",
    "            label='LightGBM', linewidth=2.5, color='steelblue', alpha=0.9,\n",
    "            marker='o' if is_binary else None, markersize=8)\n",
    "    \n",
    "    ax.plot(data['grid_logit'], data['ale_logit'], \n",
    "            label='Logistic', linewidth=2.5, color='orange', alpha=0.9, \n",
    "            linestyle='--', marker='s' if is_binary else None, markersize=8)\n",
    "    \n",
    "    ax.axhline(y=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Fix matplotlib auto-scaling: explicitly set x-limits to show full range\n",
    "    if not is_binary:\n",
    "        all_x = np.concatenate([data['grid_lgbm'], data['grid_logit']])\n",
    "        x_padding = (all_x.max() - all_x.min()) * 0.05\n",
    "        ax.set_xlim(all_x.min() - x_padding, all_x.max() + x_padding)\n",
    "    \n",
    "    lower_bound, upper_bound = data['bounds']\n",
    "    if is_binary:\n",
    "        ax.set_title(f\"{feature}*\\n(Binary: 0 vs 1)\", fontsize=9, fontweight='bold')\n",
    "        ax.set_xticks(data['grid_lgbm'])\n",
    "        ax.set_xticklabels([f\"{int(v)}\" for v in data['grid_lgbm']])\n",
    "    else:\n",
    "        ax.set_title(f\"{feature}\\n[{lower_bound:.2f}, {upper_bound:.2f}]\", fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('ALE (\u0394 PD)', fontsize=8)\n",
    "    ax.tick_params(labelsize=7)\n",
    "    ax.grid(alpha=0.2)\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "\n",
    "for idx in range(n_plots, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'FIGURE 2: ALE Plots - LightGBM vs Logistic Regression (5th-95th Percentile)\\n' + \n",
    "             f'Top {n_plots} Continuous Features', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fqcm8ffqn",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.3 SHAP Summary Visualizations\n",
    "\n",
    "# Split into two separate figures for better readability\n",
    "print(\"Creating SHAP summary plots (split into two figures for readability)...\")\n",
    "\n",
    "# FIGURE 3A: LightGBM SHAP Summary\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "shap.summary_plot(\n",
    "    shap_values_lgbm,\n",
    "    X_val,\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.title('LightGBM - SHAP Summary (Beeswarm)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.suptitle('FIGURE 3A: LightGBM Feature Importance and Impact Direction',\n",
    "             fontsize=15, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# FIGURE 3B: Logistic Regression SHAP Summary\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "shap.summary_plot(\n",
    "    shap_values_logit,\n",
    "    X_val,  # Use original feature names\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.title('Logistic Regression - SHAP Summary (Beeswarm)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.suptitle('FIGURE 3B: Logistic Regression Feature Importance and Impact Direction',\n",
    "             fontsize=15, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nREADING THE BEESWARM PLOTS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"  \u2022 Y-axis: Features (sorted by importance)\")\n",
    "print(\"  \u2022 X-axis: SHAP value (impact on model output)\")\n",
    "print(\"  \u2022 Color: Feature value (red=high, blue=low)\")\n",
    "print(\"  \u2022 Each dot: One observation\")\n",
    "print(\"  \u2022 Spread: Distribution of impacts across dataset\")\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"  - Red dots on right: High feature value \u2192 increases risk\")\n",
    "print(\"  - Blue dots on left: Low feature value \u2192 decreases risk\")\n",
    "print(\"  - Wide spread: Feature has variable impact\")\n",
    "print(\"\\nNote: Plots are now split into separate figures for better readability and thesis formatting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tpodc0iqv6s",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHAP Interaction Values\n",
    "\n",
    "# Use a random sample for faster computation\n",
    "np.random.seed(42)\n",
    "sample_size = min(10000, len(X_val))\n",
    "sample_idx = np.random.choice(len(X_val), size=sample_size, replace=False)\n",
    "X_val_sample = X_val.iloc[sample_idx]\n",
    "\n",
    "print(f\"Computing SHAP interaction values on {sample_size:,} samples...\")\n",
    "\n",
    "# Compute proper SHAP interaction values using TreeExplainer\n",
    "shap_interaction_values = explainer_lgbm.shap_interaction_values(X_val_sample)\n",
    "\n",
    "# For binary classification, TreeExplainer returns [neg_class, pos_class]\n",
    "if isinstance(shap_interaction_values, list):\n",
    "    shap_interaction_values = shap_interaction_values[1]\n",
    "\n",
    "print(f\"\u2713 SHAP interaction values computed (shape: {shap_interaction_values.shape})\")\n",
    "\n",
    "# shap_interaction_values shape: (n_samples, n_features, n_features)\n",
    "# shap_interaction_values[i, j, k] = interaction effect between features j and k for sample i\n",
    "\n",
    "# Vectorized computation: mean absolute interaction strength for each feature pair\n",
    "interaction_matrix = np.abs(shap_interaction_values).mean(axis=0)\n",
    "np.fill_diagonal(interaction_matrix, 0)  # Exclude self-interactions\n",
    "\n",
    "# For each feature, find its strongest interaction partner\n",
    "feature_names = X_val.columns.tolist()\n",
    "top_10_features = importance_comparison.head(10)['feature'].tolist()\n",
    "\n",
    "interaction_summary = []\n",
    "for feature in top_10_features:\n",
    "    feat_idx = X_val.columns.get_loc(feature)\n",
    "    \n",
    "    # Find strongest interaction\n",
    "    strongest_idx = np.argmax(interaction_matrix[feat_idx, :])\n",
    "    strongest_strength = interaction_matrix[feat_idx, strongest_idx]\n",
    "    \n",
    "    interaction_summary.append({\n",
    "        'feature': feature,\n",
    "        'top_interacting_feature': feature_names[strongest_idx],\n",
    "        'interaction_strength': strongest_strength\n",
    "    })\n",
    "\n",
    "interaction_df = pd.DataFrame(interaction_summary)\n",
    "interaction_df = interaction_df.sort_values('interaction_strength', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE: SHAP Interaction Strengths (Top 10 Features)\")\n",
    "print(\"=\" * 80)\n",
    "print(interaction_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  \u2022 Interaction strength = mean |SHAP interaction value|\")\n",
    "print(\"  \u2022 Measures how much feature A's effect depends on feature B\")\n",
    "print(\"  \u2022 Values < 0.002 (~10% of main effects) are generally negligible\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if interactions are significant\n",
    "max_interaction = interaction_df['interaction_strength'].max()\n",
    "top_main_effect = importance_comparison.head(1)['lgbm_shap'].values[0]\n",
    "interaction_ratio = max_interaction / top_main_effect\n",
    "\n",
    "print(f\"\\nSignificance assessment:\")\n",
    "print(f\"  \u2022 Strongest interaction: {max_interaction:.4f}\")\n",
    "print(f\"  \u2022 Top main effect (SHAP): {top_main_effect:.4f}\")\n",
    "print(f\"  \u2022 Ratio: {interaction_ratio*100:.1f}%\")\n",
    "\n",
    "if interaction_ratio < 0.10:\n",
    "    print(f\"  \u2192 Interactions are NEGLIGIBLE (<10% of main effects)\")\n",
    "    print(f\"  \u2192 Model relies primarily on additive feature contributions\")\n",
    "elif interaction_ratio < 0.30:\n",
    "    print(f\"  \u2192 Interactions are MODERATE (10-30% of main effects)\")\n",
    "else:\n",
    "    print(f\"  \u2192 Interactions are SIGNIFICANT (>30% of main effects)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization: Heatmap of interaction matrix for top features\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "# Get indices of top 15 features\n",
    "top_15_features = importance_comparison.head(15)['feature'].tolist()\n",
    "top_15_indices = [X_val.columns.get_loc(f) for f in top_15_features]\n",
    "\n",
    "# Extract submatrix for top features\n",
    "interaction_submatrix = interaction_matrix[np.ix_(top_15_indices, top_15_indices)]\n",
    "\n",
    "# Create heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Custom colormap: white to blue\n",
    "cmap = LinearSegmentedColormap.from_list('interaction', ['white', 'lightblue', 'steelblue', 'darkblue'])\n",
    "\n",
    "im = ax.imshow(interaction_submatrix, cmap=cmap, aspect='auto')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(top_15_features)))\n",
    "ax.set_yticks(np.arange(len(top_15_features)))\n",
    "ax.set_xticklabels(top_15_features, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(top_15_features, fontsize=9)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Mean |SHAP interaction|', rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "# Add title\n",
    "ax.set_title('FIGURE 4: SHAP Interaction Heatmap (Top 15 Features)\\nLightGBM Model', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.set_xticks(np.arange(len(top_15_features)) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(top_15_features)) - 0.5, minor=True)\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nReading the heatmap:\")\n",
    "print(\"  \u2022 Darker colors = stronger interactions\")\n",
    "print(\"  \u2022 Diagonal (self-interaction) excluded from analysis\")\n",
    "print(\"  \u2022 Matrix is symmetric (interaction between A and B = interaction between B and A)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}