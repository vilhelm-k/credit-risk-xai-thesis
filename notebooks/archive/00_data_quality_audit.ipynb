{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Audit\n",
    "\n",
    "Comprehensive analysis of feature engineering pipeline data quality:\n",
    "- Data types and memory efficiency\n",
    "- Extreme outliers and impossible values\n",
    "- Near-zero division issues\n",
    "- Categorical feature handling\n",
    "- Missing value patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nPROJ_ROOT = Path.cwd().parent\nif str(PROJ_ROOT) not in sys.path:\n    sys.path.append(str(PROJ_ROOT))\n\nfrom credit_risk_xai.config import FEATURE_CACHE_PATH, FEATURES_FOR_MODEL, CATEGORICAL_COLS\nfrom credit_risk_xai.features.engineer import prepare_modeling_data\nfrom credit_risk_xai.plotting import set_thesis_style, FIGSIZE, COLORS, save_figure, despine\n\n# Initialize thesis-quality plotting style\nset_thesis_style(use_tex=True)\n\nFIGURES_DIR = PROJ_ROOT / \"figures\"\n\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Basic Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-engineered dataset\n",
    "df = pd.read_parquet(FEATURE_CACHE_PATH)\n",
    "\n",
    "# Apply standard filters\n",
    "df_filtered = df[\n",
    "    (df['ser_aktiv'] == 1) & \n",
    "    (df['sme_category'].isin(['Small', 'Medium'])) & \n",
    "    (df['knc_kncfall'] == 1) &\n",
    "    (df[\"bransch_borsbransch_konv\"] != \"40.0\")\n",
    "]\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"After filters: {len(df_filtered):,}\")\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(f\"  Full dataset: {df.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "print(f\"  Filtered: {df_filtered.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Get modeling features\n",
    "X, y = prepare_modeling_data(df_filtered)\n",
    "print(f\"\\nModeling data: {len(X):,} rows \u00d7 {len(X.columns)} features\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicates = df_filtered[\n",
    "    (df_filtered['target'] == 1) &\n",
    "    (df_filtered['ny_omsf'] == 0.0)\n",
    "    ]\n",
    "df_duplicates.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"DATA TYPE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze dtypes\n",
    "dtype_summary = X.dtypes.value_counts()\n",
    "print(\"\\nData type distribution:\")\n",
    "print(dtype_summary)\n",
    "\n",
    "# Check categorical dtypes\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "categorical_status = []\n",
    "for col in CATEGORICAL_COLS:\n",
    "    if col in X.columns:\n",
    "        is_category = X[col].dtype.name == 'category'\n",
    "        n_unique = X[col].nunique()\n",
    "        categorical_status.append({\n",
    "            'feature': col,\n",
    "            'current_dtype': str(X[col].dtype),\n",
    "            'is_category': '\u2713' if is_category else '\u2717',\n",
    "            'unique_values': n_unique,\n",
    "            'null_pct': f\"{100 * X[col].isna().sum() / len(X):.1f}%\"\n",
    "        })\n",
    "\n",
    "cat_df = pd.DataFrame(categorical_status)\n",
    "print(cat_df.to_string(index=False))\n",
    "\n",
    "# Dtype efficiency check\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"DTYPE EFFICIENCY ISSUES\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for col in X.columns:\n",
    "    dtype = X[col].dtype\n",
    "    \n",
    "    # Check if using oversized int\n",
    "    if dtype == 'Int16' or dtype == 'int16':\n",
    "        min_val, max_val = X[col].min(), X[col].max()\n",
    "        if min_val >= 0 and max_val <= 255:\n",
    "            print(f\"  {col}: Using {dtype}, but range [{min_val}, {max_val}] fits in uint8\")\n",
    "    \n",
    "    # Check if using float64 unnecessarily\n",
    "    if dtype == 'float64':\n",
    "        print(f\"  {col}: Using float64 (could use float32 if precision not critical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"DESCRIPTIVE STATISTICS (All Features)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Get numeric columns only\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "desc = X[numeric_cols].describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]).T\n",
    "\n",
    "# Add null percentage\n",
    "desc['null_pct'] = 100 * X[numeric_cols].isna().sum() / len(X)\n",
    "\n",
    "# Add outlier count (beyond 5 std devs)\n",
    "outlier_counts = []\n",
    "for col in numeric_cols:\n",
    "    values = X[col].dropna()\n",
    "    if len(values) > 0:\n",
    "        mean, std = values.mean(), values.std()\n",
    "        if std > 0:\n",
    "            outliers = (np.abs(values - mean) > 5 * std).sum()\n",
    "        else:\n",
    "            outliers = 0\n",
    "    else:\n",
    "        outliers = 0\n",
    "    outlier_counts.append(outliers)\n",
    "\n",
    "desc['outliers_5std'] = outlier_counts\n",
    "\n",
    "print(desc.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Near-Zero Division Issues\n",
    "\n",
    "Identify features with extreme values likely caused by division by near-zero denominators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"NEAR-ZERO DIVISION ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Features known to have division operations\n",
    "division_features = {\n",
    "    'dso_days': {'numerator': 'br06g_kfordsu', 'denominator': 'rr01_ntoms', 'reasonable_range': (0, 365)},\n",
    "    'dpo_days': {'numerator': 'br13a_ksklev', 'denominator': 'rr06a_prodkos', 'reasonable_range': (0, 365)},\n",
    "    'ratio_cash_liquidity': {'numerator': 'br08_omstgsu', 'denominator': 'br13_ksksu', 'reasonable_range': (0, 10)},\n",
    "    'ratio_cash_interest_cov': {'numerator': None, 'denominator': None, 'reasonable_range': (-100, 100)},\n",
    "    'rr07_rorresul_yoy_pct': {'numerator': None, 'denominator': 'prev_year_value', 'reasonable_range': (-100, 100)},\n",
    "    'ratio_cash_liquidity_yoy_pct': {'numerator': None, 'denominator': 'prev_year_value', 'reasonable_range': (-100, 100)},\n",
    "    'current_ratio_yoy_pct': {'numerator': None, 'denominator': 'prev_year_value', 'reasonable_range': (-100, 100)},\n",
    "}\n",
    "\n",
    "for feature, info in division_features.items():\n",
    "    if feature not in X.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    values = X[feature].dropna()\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"  Count: {len(values):,} | Null: {X[feature].isna().sum():,} ({100*X[feature].isna().sum()/len(X):.1f}%)\")\n",
    "    print(f\"  Min: {values.min():.2f} | Max: {values.max():.2f}\")\n",
    "    print(f\"  Mean: {values.mean():.2f} | Median: {values.median():.2f}\")\n",
    "    print(f\"  P1: {values.quantile(0.01):.2f} | P99: {values.quantile(0.99):.2f}\")\n",
    "    \n",
    "    # Check reasonable range\n",
    "    reasonable_min, reasonable_max = info['reasonable_range']\n",
    "    in_range = (values >= reasonable_min) & (values <= reasonable_max)\n",
    "    pct_in_range = 100 * in_range.sum() / len(values)\n",
    "    \n",
    "    print(f\"  In reasonable range [{reasonable_min}, {reasonable_max}]: {pct_in_range:.1f}%\")\n",
    "    print(f\"  Outside reasonable range: {(~in_range).sum():,} obs ({100*(~in_range).sum()/len(values):.1f}%)\")\n",
    "    \n",
    "    # Extreme values\n",
    "    extreme_low = values < (reasonable_min - 1000)\n",
    "    extreme_high = values > (reasonable_max + 1000)\n",
    "    \n",
    "    if extreme_low.sum() > 0:\n",
    "        print(f\"  \u26a0 EXTREME LOW (<{reasonable_min - 1000}): {extreme_low.sum():,} obs\")\n",
    "        print(f\"    Worst 5: {values[extreme_low].nsmallest(5).values}\")\n",
    "    \n",
    "    if extreme_high.sum() > 0:\n",
    "        print(f\"  \u26a0 EXTREME HIGH (>{reasonable_max + 1000}): {extreme_high.sum():,} obs\")\n",
    "        print(f\"    Worst 5: {values[extreme_high].nlargest(5).values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Denominator Analysis\n",
    "\n",
    "Check the distributions of denominators to understand near-zero division issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"DENOMINATOR ANALYSIS (for division-based features)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check key denominators\n",
    "denominators = {\n",
    "    'rr01_ntoms': 'Revenue (used in DSO calculation)',\n",
    "    'rr06a_prodkos': 'Production costs (used in DPO calculation)',\n",
    "    'br13_ksksu': 'Current liabilities (used in cash liquidity ratio)',\n",
    "}\n",
    "\n",
    "for denom_col, description in denominators.items():\n",
    "    if denom_col not in df_filtered.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{denom_col}: {description}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    values = df_filtered[denom_col].dropna()\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"  Count: {len(values):,}\")\n",
    "    print(f\"  Min: {values.min():.2f} | Max: {values.max():.2f}\")\n",
    "    print(f\"  Mean: {values.mean():.2f} | Median: {values.median():.2f}\")\n",
    "    \n",
    "    # Check for near-zero and negative\n",
    "    zero_count = (values == 0).sum()\n",
    "    near_zero_1 = (values.abs() < 1).sum()\n",
    "    near_zero_10 = (values.abs() < 10).sum()\n",
    "    negative = (values < 0).sum()\n",
    "    \n",
    "    print(f\"\\n  Zero: {zero_count:,} ({100*zero_count/len(values):.2f}%)\")\n",
    "    print(f\"  |value| < 1: {near_zero_1:,} ({100*near_zero_1/len(values):.2f}%)\")\n",
    "    print(f\"  |value| < 10: {near_zero_10:,} ({100*near_zero_10/len(values):.2f}%)\")\n",
    "    print(f\"  Negative: {negative:,} ({100*negative/len(values):.2f}%)\")\n",
    "    \n",
    "    # Recommended threshold\n",
    "    if 'ntoms' in denom_col:  # Revenue\n",
    "        threshold = 10.0\n",
    "    else:\n",
    "        threshold = 1.0\n",
    "    \n",
    "    below_threshold = (values.abs() < threshold).sum()\n",
    "    print(f\"\\n  \u26a0 WOULD EXCLUDE with threshold={threshold}: {below_threshold:,} ({100*below_threshold/len(values):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: Extreme Value Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot distributions of problematic features\nproblematic_features = [\n    'dso_days', 'dpo_days', 'ratio_cash_liquidity', \n    'rr07_rorresul_yoy_pct', 'ratio_cash_liquidity_yoy_pct'\n]\n\nfig, axes = plt.subplots(2, 3, figsize=(FIGSIZE['standalone'][0], 3.5))\naxes = axes.flatten()\n\nfor idx, feature in enumerate(problematic_features):\n    if feature not in X.columns or idx >= len(axes):\n        continue\n    \n    ax = axes[idx]\n    values = X[feature].dropna()\n    \n    # Use percentile range for visualization\n    p1, p99 = values.quantile([0.01, 0.99])\n    filtered_values = values[(values >= p1) & (values <= p99)]\n    \n    # Histogram\n    ax.hist(filtered_values, bins=50, alpha=0.7, edgecolor='white', linewidth=0.3,\n            color=COLORS['lgbm'])\n    ax.set_title(feature.replace('_', r'\\_'), fontsize=8)\n    ax.set_xlabel('Value', fontsize=7)\n    ax.set_ylabel('Frequency', fontsize=7)\n    ax.tick_params(labelsize=6)\n    \n    # Add stats text\n    stats_text = f\"P1--P99: [{p1:.1f}, {p99:.1f}]\\nOutside: {(~values.between(p1, p99)).sum():,}\"\n    ax.text(0.98, 0.98, stats_text, transform=ax.transAxes,\n            verticalalignment='top', horizontalalignment='right',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='#cccccc'),\n            fontsize=5)\n    \n    despine(ax)\n\n# Hide extra subplot\nif len(problematic_features) < len(axes):\n    axes[-1].axis('off')\n\nplt.tight_layout()\nsave_figure(fig, FIGURES_DIR / 'feature_distributions_audit.pdf')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"SUMMARY: REQUIRED FIXES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n1. CATEGORICAL DTYPES\")\n",
    "print(\"-\" * 80)\n",
    "cat_not_encoded = [col for col in CATEGORICAL_COLS if col in X.columns and X[col].dtype.name != 'category']\n",
    "if cat_not_encoded:\n",
    "    print(f\"  \u2717 {len(cat_not_encoded)} categorical features not using category dtype:\")\n",
    "    for col in cat_not_encoded:\n",
    "        print(f\"    - {col}: {X[col].dtype}\")\n",
    "else:\n",
    "    print(\"  \u2713 All categorical features properly encoded\")\n",
    "\n",
    "print(\"\\n2. NEAR-ZERO DIVISION ISSUES\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  Recommend implementing threshold-based NaN conversion in _safe_div():\")\n",
    "print(\"    - min_abs_denom=1.0 for most ratios (1k SEK)\")\n",
    "print(\"    - min_abs_denom=10.0 for revenue-based metrics (10k SEK)\")\n",
    "print(\"\\n  Expected impact:\")\n",
    "\n",
    "# Estimate impact for key denominators\n",
    "if 'rr01_ntoms' in df_filtered.columns:\n",
    "    revenue_below_10 = (df_filtered['rr01_ntoms'].abs() < 10).sum()\n",
    "    print(f\"    - DSO: ~{100*revenue_below_10/len(df_filtered):.1f}% \u2192 NaN (revenue < 10k)\")\n",
    "\n",
    "if 'rr06a_prodkos' in df_filtered.columns:\n",
    "    prodcost_below_1 = (df_filtered['rr06a_prodkos'].abs() < 1).sum()\n",
    "    print(f\"    - DPO: ~{100*prodcost_below_1/len(df_filtered):.1f}% \u2192 NaN (prod costs < 1k)\")\n",
    "\n",
    "if 'br13_ksksu' in df_filtered.columns:\n",
    "    liab_below_1 = (df_filtered['br13_ksksu'].abs() < 1).sum()\n",
    "    print(f\"    - Cash liquidity ratio: ~{100*liab_below_1/len(df_filtered):.1f}% \u2192 NaN (liabilities < 1k)\")\n",
    "\n",
    "print(\"\\n3. DATA TYPE OPTIMIZATION\")\n",
    "print(\"-\" * 80)\n",
    "if 'event_count_last_5y' in X.columns:\n",
    "    print(f\"  \u2022 event_count_last_5y: {X['event_count_last_5y'].dtype} \u2192 uint8 (range 0-5)\")\n",
    "float64_cols = [col for col in X.columns if X[col].dtype == 'float64']\n",
    "if float64_cols:\n",
    "    print(f\"  \u2022 {len(float64_cols)} features using float64 could use float32\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\\n\u2713 Data quality audit complete. Proceed with fixes in engineer.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}