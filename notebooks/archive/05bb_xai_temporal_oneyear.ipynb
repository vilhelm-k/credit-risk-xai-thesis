{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Temporal Evolution of Credit Risk Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PROJ_ROOT = Path.cwd().parent\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJ_ROOT))\n",
    "\n",
    "from credit_risk_xai.config import (\n",
    "    FEATURE_CACHE_PATH, ACTIVE_MODEL_VERSION, ACTIVE_FEATURES, get_display_name, FEATURE_BOUNDS\n",
    ")\n",
    "from credit_risk_xai.features.engineer import prepare_modeling_data\n",
    "from credit_risk_xai.modeling.train import DEFAULT_PARAMS\n",
    "from credit_risk_xai.modeling import compute_ale_for_feature, compute_ale_binary, compute_ece\n",
    "from credit_risk_xai.plotting import (\n",
    "    set_thesis_style, COLORS, FIGSIZE, save_figure, despine,\n",
    ")\n",
    "from src.xai_utils import calculate_shap_importance, compute_performance_metrics\n",
    "\n",
    "# Initialize thesis-quality plotting style\n",
    "set_thesis_style(use_tex=True)\n",
    "\n",
    "FIGURES_DIR = PROJ_ROOT / \"figures\"\n",
    "\n",
    "TEMPORAL_FEATURES = [f for f in ACTIVE_FEATURES if f != 'term_spread']\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION: Set to True to include industry code, False to exclude\n",
    "# =============================================================================\n",
    "INCLUDE_INDUSTRY_CODE = False  # Toggle this to include/exclude sni_group_3digit\n",
    "\n",
    "if not INCLUDE_INDUSTRY_CODE:\n",
    "    TEMPORAL_FEATURES = [f for f in TEMPORAL_FEATURES if f != 'sni_group_3digit']\n",
    "\n",
    "print(f\"Model version: {ACTIVE_MODEL_VERSION}\")\n",
    "print(f\"Features for temporal analysis: {len(TEMPORAL_FEATURES)}\")\n",
    "if not INCLUDE_INDUSTRY_CODE:\n",
    "    print(\"  (sni_group_3digit excluded - set INCLUDE_INDUSTRY_CODE=True to include)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Economic Periods (Single-Year Training)\n",
    "\n",
    "We analyze 6 distinct economic periods. For each period, we train on **exactly one year** and test on the **next year**. Since the target variable is *next-year default*, the training year contains features predicting defaults in the test year, providing a clean temporal separation.\n",
    "\n",
    "| Period | Train Year | Test Year | Economic Context |\n",
    "|--------|------------|-----------|------------------|\n",
    "| Pre-Crisis | 2005 | 2006 | Stable pre-crisis conditions |\n",
    "| Financial Crisis | 2008 | 2009 | Peak of Global Financial Crisis |\n",
    "| Sovereign Debt | 2011 | 2012 | European sovereign debt crisis |\n",
    "| Recovery | 2015 | 2016 | Post-crisis recovery period |\n",
    "| COVID | 2019 | 2020 | Pre-COVID features predicting COVID-era defaults |\n",
    "| Post-COVID | 2021 | 2022 | Post-pandemic normalization/inflation shock |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-year training approach: (train_year, test_year)\n",
    "PERIODS = {\n",
    "    'pre_crisis': (2005, 2006),\n",
    "    'financial_crisis': (2008, 2009),\n",
    "    'sovereign_debt': (2011, 2012),\n",
    "    'recovery': (2015, 2016),\n",
    "    'covid': (2019, 2020),\n",
    "    'post_covid': (2021, 2022)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Create Period-Specific Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(FEATURE_CACHE_PATH)\n",
    "df = df[\n",
    "    (df[\"ser_aktiv\"] == 1) & \n",
    "    (df[\"sme_category\"].isin([\"Small\", \"Medium\"])) & \n",
    "    (df[\"knc_kncfall\"] == 1) &\n",
    "    (df[\"bransch_borsbransch_konv\"] != \"40.0\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's understand the data availability\n",
    "print(\"Available years in dataset:\", sorted(df['ser_year'].unique()))\n",
    "print()\n",
    "\n",
    "# Check target availability by year\n",
    "target_by_year = df.groupby('ser_year').agg({\n",
    "    'target_next_year': ['count', lambda x: x.notna().sum()]\n",
    "}).round(0)\n",
    "target_by_year.columns = ['total_rows', 'valid_targets']\n",
    "print(\"Target availability by year:\")\n",
    "print(target_by_year)\n",
    "print()\n",
    "\n",
    "period_data = {}\n",
    "\n",
    "for period_name, (train_year, test_year) in PERIODS.items():\n",
    "    # Train on exactly one year, test on the next year\n",
    "    train_mask = df['ser_year'] == train_year\n",
    "    eval_mask = df['ser_year'] == test_year\n",
    "    \n",
    "    df_train_period = df[train_mask].copy()\n",
    "    df_eval_period = df[eval_mask].copy()\n",
    "    \n",
    "    # Check how many have valid targets (this is what prepare_modeling_data will filter to)\n",
    "    n_train_valid = df_train_period['target_next_year'].notna().sum()\n",
    "    n_eval_valid = df_eval_period['target_next_year'].notna().sum()\n",
    "    \n",
    "    if n_train_valid == 0 or n_eval_valid == 0:\n",
    "        print(f\"SKIPPING {period_name}: Train {train_year} has {n_train_valid} valid targets, \"\n",
    "              f\"Test {test_year} has {n_eval_valid} valid targets\")\n",
    "        continue\n",
    "    \n",
    "    # Use TEMPORAL_FEATURES (excludes sni_group_3digit)\n",
    "    X_train, y_train = prepare_modeling_data(df_train_period, features=TEMPORAL_FEATURES)\n",
    "    X_eval, y_eval = prepare_modeling_data(df_eval_period, features=TEMPORAL_FEATURES)\n",
    "    \n",
    "    # Double-check we have data after prepare_modeling_data\n",
    "    if len(X_train) == 0 or len(X_eval) == 0:\n",
    "        print(f\"SKIPPING {period_name}: After filtering - Train has {len(X_train)} rows, \"\n",
    "              f\"Test has {len(X_eval)} rows\")\n",
    "        continue\n",
    "    \n",
    "    period_data[period_name] = {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_eval': X_eval, 'y_eval': y_eval,\n",
    "        'train_year': train_year,\n",
    "        'eval_year': test_year\n",
    "    }\n",
    "    \n",
    "    print(f\"{period_name}: Train on {train_year} (n={len(X_train):,}, default rate={y_train.mean():.2%}) -> \"\n",
    "          f\"Test on {test_year} (n={len(X_eval):,}, default rate={y_eval.mean():.2%})\")\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(period_data)} periods: {list(period_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Period-Specific Models\n",
    "\n",
    "For each period, we train a LightGBM model on a single year and evaluate on the following year. This mimics a realistic deployment scenario where a model is trained on recent data and applied to predict near-future defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_models = {}\n",
    "period_predictions = {}\n",
    "period_shap = {}\n",
    "\n",
    "for period_name, data in period_data.items():\n",
    "    print(f\"Training model for {period_name}...\")\n",
    "    X_train, y_train = data['X_train'], data['y_train']\n",
    "    X_eval, y_eval = data['X_eval'], data['y_eval']\n",
    "    \n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**DEFAULT_PARAMS)\n",
    "    model.fit(\n",
    "        X_tr, y_tr, \n",
    "        eval_set=[(X_val, y_val)], \n",
    "        eval_metric='logloss',\n",
    "        categorical_feature='auto',\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_eval)[:, 1]\n",
    "    metrics = compute_performance_metrics(y_eval, y_pred_proba)\n",
    "    \n",
    "    sample_size = min(5000, len(X_eval))\n",
    "    sample_idx = np.random.choice(len(X_eval), size=sample_size, replace=False) if sample_size < len(X_eval) else np.arange(len(X_eval))\n",
    "    X_eval_sample = X_eval.iloc[sample_idx]\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_eval_sample)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    period_models[period_name] = model\n",
    "    period_predictions[period_name] = {'y_true': y_eval, 'y_pred_proba': y_pred_proba, 'metrics': metrics}\n",
    "    period_shap[period_name] = {'shap_values': shap_values, 'X_eval_sample': X_eval_sample, 'feature_names': X_eval.columns.tolist()}\n",
    "    \n",
    "    print(f\"  -> AUC: {metrics['AUC']:.3f}, PR-AUC: {metrics['PR-AUC']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Stability Analysis\n",
    "\n",
    "### 4.1 Calculate SHAP Importance for Each Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_by_period = {}\n",
    "\n",
    "for period_name, shap_data in period_shap.items():\n",
    "    importance = calculate_shap_importance(\n",
    "        shap_data['shap_values'],\n",
    "        shap_data['feature_names']\n",
    "    )\n",
    "    importance_by_period[period_name] = importance.set_index('feature')['importance']\n",
    "\n",
    "importance_evolution = pd.DataFrame(importance_by_period)\n",
    "importance_evolution = importance_evolution.fillna(0)\n",
    "\n",
    "# Use only periods that were successfully trained\n",
    "available_periods = list(period_data.keys())\n",
    "importance_evolution['mean'] = importance_evolution[available_periods].mean(axis=1)\n",
    "importance_evolution['std'] = importance_evolution[available_periods].std(axis=1)\n",
    "importance_evolution['cv'] = importance_evolution['std'] / importance_evolution['mean']\n",
    "importance_evolution['cv'] = importance_evolution['cv'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "importance_evolution = importance_evolution.sort_values('mean', ascending=False)\n",
    "importance_evolution.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualize Importance Evolution Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Rank Heatmap Over Time\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE['full_tall'])\n",
    "\n",
    "top_features = importance_evolution.index.tolist()\n",
    "available_periods = list(period_data.keys())\n",
    "\n",
    "# Compute ranks for each period\n",
    "rank_matrix = pd.DataFrame(index=top_features, columns=available_periods)\n",
    "for period_name in available_periods:\n",
    "    ranks = importance_evolution[period_name].rank(ascending=False)\n",
    "    rank_matrix[period_name] = ranks[top_features]\n",
    "\n",
    "# Human-readable feature names\n",
    "display_labels = [get_display_name(f) for f in top_features]\n",
    "\n",
    "# Clean period labels - updated for single-year training\n",
    "period_display_names = {\n",
    "    'pre_crisis': 'Pre-Crisis',\n",
    "    'financial_crisis': 'Financial\\nCrisis',\n",
    "    'sovereign_debt': 'Sovereign\\nDebt',\n",
    "    'recovery': 'Recovery',\n",
    "    'covid': 'COVID',\n",
    "    'post_covid': 'Post-\\nCOVID'\n",
    "}\n",
    "\n",
    "sns.heatmap(\n",
    "    rank_matrix.astype(float),\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    annot_kws={'size': 8},\n",
    "    cmap='RdYlGn_r',\n",
    "    cbar_kws={'label': 'Importance Rank', 'shrink': 0.8},\n",
    "    ax=ax,\n",
    "    vmin=1,\n",
    "    vmax=len(top_features),\n",
    "    yticklabels=display_labels,\n",
    "    linewidths=0.5,\n",
    "    linecolor='white'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_xticklabels([period_display_names[p] for p in available_periods], rotation=0, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, FIGURES_DIR / 'temporal_importance_ranks_oneyear.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Comparison Across Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_periods = list(period_data.keys())\n",
    "\n",
    "performance_comparison = pd.DataFrame({\n",
    "    'Period': available_periods,\n",
    "    'Train Year': [period_data[p]['train_year'] for p in available_periods],\n",
    "    'Test Year': [period_data[p]['eval_year'] for p in available_periods],\n",
    "    'AUC': [period_predictions[p]['metrics']['AUC'] for p in available_periods],\n",
    "    'PR-AUC': [period_predictions[p]['metrics']['PR-AUC'] for p in available_periods],\n",
    "    'Brier': [period_predictions[p]['metrics']['Brier Score'] for p in available_periods],\n",
    "    'ECE': [period_predictions[p]['metrics']['ECE'] for p in available_periods],\n",
    "    'Train Default Rate': [period_data[p]['y_train'].mean() for p in available_periods],\n",
    "    'Test Default Rate': [period_predictions[p]['y_true'].mean() for p in available_periods]\n",
    "})\n",
    "performance_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ALE Plot Evolution (Threshold Shifts)\n",
    "\n",
    "Investigate how risk relationships change across periods for key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_for_ale = [\n",
    "    f for f in importance_evolution.head(6).index.tolist()\n",
    "    if period_data[list(PERIODS.keys())[0]]['X_eval'][f].dtype != 'category'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ale_evolution = {}\n",
    "available_periods = list(period_data.keys())\n",
    "\n",
    "all_features = [f for f in period_data[available_periods[0]]['X_eval'].columns \n",
    "                if period_data[available_periods[0]]['X_eval'][f].dtype != 'category']\n",
    "\n",
    "binary_features = [f for f in all_features if FEATURE_BOUNDS.get(f) is None]\n",
    "continuous_features = [f for f in all_features if FEATURE_BOUNDS.get(f) is not None]\n",
    "\n",
    "for i, feature in enumerate(binary_features):\n",
    "    print(f\"Computing ALE for binary feature: {i+1}/{len(binary_features)} {feature}\")\n",
    "    ale_evolution[feature] = {'is_binary': True}\n",
    "    \n",
    "    for period_name in available_periods:\n",
    "        model = period_models[period_name]\n",
    "        X_eval = period_data[period_name]['X_eval']\n",
    "        \n",
    "        try:\n",
    "            grid, ale, bounds = compute_ale_binary(model.predict_proba, X_eval, feature)\n",
    "            ale_evolution[feature][period_name] = {'grid': grid, 'ale': ale, 'bounds': bounds}\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "for i, feature in enumerate(continuous_features):\n",
    "    print(f\"Computing ALE for continuous feature: {i+1}/{len(continuous_features)} {feature}\")\n",
    "    ale_evolution[feature] = {}\n",
    "    bounds = FEATURE_BOUNDS.get(feature)\n",
    "    \n",
    "    for period_name in available_periods:\n",
    "        model = period_models[period_name]\n",
    "        X_eval = period_data[period_name]['X_eval']\n",
    "        \n",
    "        try:\n",
    "            grid, ale, bounds_used = compute_ale_for_feature(model.predict_proba, X_eval, feature, feature_bounds=bounds)\n",
    "            ale_evolution[feature][period_name] = {'grid': grid, 'ale': ale, 'bounds': bounds_used}\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALE Evolution Grid - All Features Across Economic Periods\n",
    "n_features = len(ale_evolution)\n",
    "n_cols = 4\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "available_periods = list(period_data.keys())\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 2.8 * n_rows))\n",
    "axes = axes.flatten() if n_features > 1 else [axes]\n",
    "\n",
    "# Period colors and labels\n",
    "period_colors = {\n",
    "    'pre_crisis': '#1f77b4',\n",
    "    'financial_crisis': '#ff7f0e',\n",
    "    'sovereign_debt': '#2ca02c',\n",
    "    'recovery': '#d62728',\n",
    "    'covid': '#9467bd',\n",
    "    'post_covid': '#8c564b'\n",
    "}\n",
    "period_labels = {\n",
    "    'pre_crisis': \"'05$\\\\to$'06\",\n",
    "    'financial_crisis': \"'08$\\\\to$'09\",\n",
    "    'sovereign_debt': \"'11$\\\\to$'12\",\n",
    "    'recovery': \"'15$\\\\to$'16\",\n",
    "    'covid': \"'19$\\\\to$'20\",\n",
    "    'post_covid': \"'22$\\\\to$'23\"\n",
    "}\n",
    "\n",
    "for idx, (feature, feature_data) in enumerate(ale_evolution.items()):\n",
    "    ax = axes[idx]\n",
    "    display_name = get_display_name(feature)\n",
    "    is_binary_feature = feature_data.get('is_binary', False)\n",
    "    plotted_any = False\n",
    "    \n",
    "    for period_name in available_periods:\n",
    "        if period_name not in feature_data:\n",
    "            continue\n",
    "            \n",
    "        ale_data = feature_data[period_name]\n",
    "        \n",
    "        if 'grid' not in ale_data or 'ale' not in ale_data:\n",
    "            continue\n",
    "            \n",
    "        if len(ale_data['grid']) > 0 and len(ale_data['ale']) > 0:\n",
    "            is_binary = is_binary_feature or len(ale_data['grid']) <= 3\n",
    "            \n",
    "            ax.plot(\n",
    "                ale_data['grid'],\n",
    "                ale_data['ale'],\n",
    "                label=period_labels[period_name],\n",
    "                linewidth=1.5,\n",
    "                color=period_colors[period_name],\n",
    "                marker='o' if is_binary else None,\n",
    "                markersize=5,\n",
    "                alpha=0.8\n",
    "            )\n",
    "            plotted_any = True\n",
    "    \n",
    "    if plotted_any:\n",
    "        ax.axhline(y=0, color=COLORS['neutral'], linestyle=':', linewidth=0.8, alpha=0.5)\n",
    "        ax.set_xlabel(display_name, fontsize=8)\n",
    "        ax.set_ylabel(r'$\\Delta$ Default Prob.', fontsize=7)\n",
    "        ax.set_title(display_name, fontsize=8, fontweight='bold')\n",
    "        ax.tick_params(labelsize=6)\n",
    "        despine(ax)\n",
    "        \n",
    "        if idx == 0:\n",
    "            ax.legend(fontsize=5, loc='best', ncol=2)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{display_name}\\n(No variation)', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
    "        ax.set_title(display_name, fontsize=8, fontweight='bold')\n",
    "\n",
    "for idx in range(n_features, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, FIGURES_DIR / 'ale_temporal_evolution_oneyear.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Evolution Over Time\n",
    "\n",
    "Visualize how SHAP importance changes across economic periods for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Feature Importance Evolution Over Time\n",
    "n_features = len(importance_evolution)\n",
    "n_cols = 5\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "available_periods = list(period_data.keys())\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 2.5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x_positions = np.arange(len(available_periods))\n",
    "\n",
    "period_colors = {\n",
    "    'pre_crisis': '#1f77b4',\n",
    "    'financial_crisis': '#ff7f0e',\n",
    "    'sovereign_debt': '#2ca02c',\n",
    "    'recovery': '#d62728',\n",
    "    'covid': '#9467bd',\n",
    "    'post_covid': '#8c564b'\n",
    "}\n",
    "\n",
    "# Updated labels for single-year training - only for available periods\n",
    "period_short_map = {\n",
    "    'pre_crisis': \"'05$\\\\to$'06\",\n",
    "    'financial_crisis': \"'08$\\\\to$'09\",\n",
    "    'sovereign_debt': \"'11$\\\\to$'12\",\n",
    "    'recovery': \"'15$\\\\to$'16\",\n",
    "    'covid': \"'19$\\\\to$'20\",\n",
    "    'post_covid': \"'22$\\\\to$'23\"\n",
    "}\n",
    "period_short = [period_short_map[p] for p in available_periods]\n",
    "\n",
    "for idx, (feature, row) in enumerate(importance_evolution.iterrows()):\n",
    "    ax = axes[idx]\n",
    "    display_name = get_display_name(feature)\n",
    "    importance_values = [row[p] for p in available_periods]\n",
    "    \n",
    "    # Line connecting points\n",
    "    ax.plot(x_positions, importance_values, \n",
    "            linewidth=1.5, color=COLORS['lgbm'], alpha=0.6)\n",
    "    \n",
    "    # Colored markers for each period\n",
    "    for i, period in enumerate(available_periods):\n",
    "        ax.plot(i, importance_values[i], 'o', \n",
    "                markersize=6, color=period_colors[period], \n",
    "                markeredgecolor='white', markeredgewidth=0.5)\n",
    "    \n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(period_short, fontsize=6, rotation=0)\n",
    "    ax.set_ylabel(r'Mean $|\\mathrm{SHAP}|$', fontsize=7)\n",
    "    ax.set_title(display_name, fontsize=8, fontweight='bold')\n",
    "    ax.tick_params(labelsize=6)\n",
    "    despine(ax)\n",
    "    \n",
    "    # Mean reference line\n",
    "    mean_importance = np.mean(importance_values)\n",
    "    ax.axhline(y=mean_importance, color=COLORS['neutral'], linestyle=':', linewidth=0.8, alpha=0.4)\n",
    "\n",
    "for idx in range(n_features, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Grouped Feature Importance Analysis\n",
    "\n",
    "**Problem**: Individual feature importance is noisy due to limited samples per period. \n",
    "**Solution**: Aggregate features into economically meaningful groups to identify systematic patterns.\n",
    "\n",
    "Feature groups aligned with V2 (Altman/Ohlson) feature set:\n",
    "1. **Profitability** - Returns on capital, margins\n",
    "2. **Liquidity** - Cash ratios, quick ratios\n",
    "3. **Leverage** - Debt structure, solvency, working capital\n",
    "4. **Growth/Momentum** - Revenue changes, volatility\n",
    "5. **Working Capital Efficiency** - DSO, DPO\n",
    "6. **Scale** - Size proxies (log-transformed assets), company age\n",
    "7. **Behavioral** - Dividend policy\n",
    "8. **Macro** - Term spread (economic conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 Feature Groups (aligned with Altman/Ohlson feature set)\n",
    "FEATURE_GROUPS = {\n",
    "    'Profitability': [\n",
    "        'ny_avkegkap',      # Return on Equity\n",
    "        'ny_nettomarg',     # Net Profit Margin\n",
    "        'gross_margin',     # Gross Margin (new in V2)\n",
    "    ],\n",
    "    'Liquidity': [\n",
    "        'ratio_cash_liquidity',  # Cash Ratio\n",
    "        'ny_kasslikv',           # Quick Ratio\n",
    "        'interest_coverage',     # Interest Coverage (V2: EBIT-based)\n",
    "    ],\n",
    "    'Leverage': [\n",
    "        'ny_skuldgrd',           # Debt Ratio (Ohlson TL/TA)\n",
    "        'ny_solid',              # Equity Ratio\n",
    "        'working_capital_ta',    # Altman X1: Working Capital / TA\n",
    "        'retained_earnings_ta',  # Altman X2: Retained Earnings / TA\n",
    "    ],\n",
    "    'Growth_Momentum': [\n",
    "        'ny_omsf',               # Revenue Growth (YoY)\n",
    "        'revenue_cagr_3y',       # Revenue CAGR (3Y)\n",
    "        'revenue_drawdown_5y',   # Revenue Drawdown (5Y)\n",
    "        'ebitda_volatility',     # EBITDA Volatility (new in V2)\n",
    "    ],\n",
    "    'Working_Capital': [\n",
    "        'dso_days',              # Days Sales Outstanding\n",
    "        'dpo_days',              # Days Payables Outstanding\n",
    "    ],\n",
    "    'Scale': [\n",
    "        'log_total_assets',      # Ohlson Size (W)\n",
    "        'company_age',           # Company Age\n",
    "        'ny_kapomsh',            # Total Asset Turnover (Altman X5)\n",
    "    ],\n",
    "    'Behavioral': [\n",
    "        'dividend_yield',        # Dividend Payer (binary)\n",
    "    ],\n",
    "    'Macro': [\n",
    "        'term_spread',           # Yield curve spread\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Verify all V2 features are covered\n",
    "all_grouped_features = [f for group in FEATURE_GROUPS.values() for f in group]\n",
    "model_features = importance_evolution.index.tolist()\n",
    "missing_from_groups = [f for f in model_features if f not in all_grouped_features]\n",
    "extra_in_groups = [f for f in all_grouped_features if f not in model_features]\n",
    "\n",
    "if missing_from_groups:\n",
    "    print(f\"Features in model but not in groups: {missing_from_groups}\")\n",
    "if extra_in_groups:\n",
    "    print(f\"Features in groups but not in model: {extra_in_groups}\")\n",
    "if not missing_from_groups and not extra_in_groups:\n",
    "    print(f\"All {len(model_features)} features are properly grouped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_periods = list(period_data.keys())\n",
    "grouped_importance = pd.DataFrame(index=FEATURE_GROUPS.keys(), columns=available_periods)\n",
    "\n",
    "for period_name in available_periods:\n",
    "    for group_name, features in FEATURE_GROUPS.items():\n",
    "        valid_features = [f for f in features if f in importance_evolution.index]\n",
    "        if valid_features:\n",
    "            group_total = importance_evolution.loc[valid_features, period_name].sum()\n",
    "            grouped_importance.loc[group_name, period_name] = group_total\n",
    "        else:\n",
    "            grouped_importance.loc[group_name, period_name] = 0\n",
    "\n",
    "grouped_importance = grouped_importance.astype(float)\n",
    "\n",
    "grouped_importance['mean'] = grouped_importance[available_periods].mean(axis=1)\n",
    "grouped_importance['std'] = grouped_importance[available_periods].std(axis=1)\n",
    "grouped_importance['cv'] = grouped_importance['std'] / grouped_importance['mean']\n",
    "\n",
    "grouped_importance = grouped_importance.sort_values('mean', ascending=False)\n",
    "grouped_importance.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Group Importance Evolution - Standalone Figure\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE['standalone'])\n",
    "available_periods = list(period_data.keys())\n",
    "\n",
    "group_colors = {\n",
    "    'Behavioral': '#e41a1c',\n",
    "    'Scale_Industry': '#377eb8',\n",
    "    'Liquidity': '#4daf4a',\n",
    "    'Leverage': '#984ea3',\n",
    "    'Growth_Momentum': '#ff7f00',\n",
    "    'Profitability': '#f781bf',\n",
    "    'Working_Capital': '#a65628',\n",
    "    'Macro': '#999999',\n",
    "}\n",
    "\n",
    "# Cleaner group names for legend\n",
    "group_display_names = {\n",
    "    'Scale_Industry': 'Scale/Industry',\n",
    "    'Growth_Momentum': 'Growth/Momentum',\n",
    "    'Working_Capital': 'Working Capital',\n",
    "}\n",
    "\n",
    "x_positions = np.arange(len(available_periods))\n",
    "\n",
    "# Period display labels - updated for single-year training\n",
    "period_display_map = {\n",
    "    'pre_crisis': \"'05$\\\\to$'06\",\n",
    "    'financial_crisis': \"'08$\\\\to$'09\",\n",
    "    'sovereign_debt': \"'11$\\\\to$'12\",\n",
    "    'recovery': \"'15$\\\\to$'16\",\n",
    "    'covid': \"'19$\\\\to$'20\",\n",
    "    'post_covid': \"'22$\\\\to$'23\"\n",
    "}\n",
    "\n",
    "for group_name in grouped_importance.index:\n",
    "    values = grouped_importance.loc[group_name, available_periods].values\n",
    "    display_name = group_display_names.get(group_name, group_name)\n",
    "    ax.plot(x_positions, values, \n",
    "            marker='o', linewidth=1.5, markersize=5,\n",
    "            label=display_name, color=group_colors.get(group_name, 'gray'),\n",
    "            alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels([period_display_map[p] for p in available_periods])\n",
    "ax.set_ylabel(r'Mean Absolute Contribution (Log-Odds)')\n",
    "ax.set_xlabel('')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0, frameon=True)\n",
    "despine(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, FIGURES_DIR / 'temporal_group_importance_oneyear.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Detailed Temporal Pattern Analysis\n",
    "\n",
    "Let's examine specific patterns more closely to understand the economic intuition behind the importance shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Temporal Pattern Analysis by Feature Group\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "axes = axes.flatten()\n",
    "available_periods = list(period_data.keys())\n",
    "\n",
    "# Updated period labels for single-year training\n",
    "period_labels = {\n",
    "    'pre_crisis': \"'05$\\\\to$'06\",\n",
    "    'financial_crisis': \"'08$\\\\to$'09\",\n",
    "    'sovereign_debt': \"'11$\\\\to$'12\",\n",
    "    'recovery': \"'15$\\\\to$'16\",\n",
    "    'covid': \"'19$\\\\to$'20\",\n",
    "    'post_covid': \"'22$\\\\to$'23\"\n",
    "}\n",
    "\n",
    "for idx, (group_name, features) in enumerate(FEATURE_GROUPS.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    valid_features = [f for f in features if f in importance_evolution.index]\n",
    "    \n",
    "    if not valid_features:\n",
    "        ax.text(0.5, 0.5, f'{group_name}\\n(No features)', ha='center', va='center')\n",
    "        continue\n",
    "    \n",
    "    for feature in valid_features:\n",
    "        display_name = get_display_name(feature)\n",
    "        values = importance_evolution.loc[feature, available_periods].values\n",
    "        ax.plot(range(len(available_periods)), values, \n",
    "                marker='o', linewidth=1.2, markersize=4,\n",
    "                label=display_name[:15], alpha=0.6)\n",
    "    \n",
    "    # Group average (bold)\n",
    "    group_avg = importance_evolution.loc[valid_features, available_periods].mean(axis=0)\n",
    "    ax.plot(range(len(available_periods)), group_avg.values, \n",
    "            linewidth=2, color='black', linestyle='--',\n",
    "            label='Group Avg', alpha=0.9)\n",
    "    \n",
    "    ax.set_xticks(range(len(available_periods)))\n",
    "    ax.set_xticklabels([period_labels[p] for p in available_periods], fontsize=6)\n",
    "    ax.set_ylabel(r'Mean $|\\mathrm{SHAP}|$', fontsize=7)\n",
    "    \n",
    "    # Clean group name for title\n",
    "    clean_name = group_name.replace('_', ' ')\n",
    "    ax.set_title(f'{clean_name}\\n({len(valid_features)} features)', fontsize=8, fontweight='bold')\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.legend(fontsize=5, loc='best', ncol=2)\n",
    "    despine(ax)\n",
    "\n",
    "for idx in range(len(FEATURE_GROUPS), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Correlation Between Feature Groups\n",
    "available_periods = list(period_data.keys())\n",
    "group_temporal_corr = grouped_importance[available_periods].T.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE['standalone'])\n",
    "\n",
    "# Lower triangle mask\n",
    "mask = np.triu(np.ones_like(group_temporal_corr, dtype=bool), k=1)\n",
    "\n",
    "# Clean group names for display\n",
    "clean_names = [n.replace('_', ' ') for n in group_temporal_corr.index]\n",
    "\n",
    "sns.heatmap(group_temporal_corr, annot=True, fmt='.2f', \n",
    "            cmap='RdBu_r', center=0, ax=ax,\n",
    "            mask=mask, square=True,\n",
    "            vmin=-1, vmax=1,\n",
    "            annot_kws={'size': 8},\n",
    "            xticklabels=clean_names,\n",
    "            yticklabels=clean_names,\n",
    "            cbar_kws={'label': 'Correlation', 'shrink': 0.8})\n",
    "\n",
    "ax.tick_params(labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, FIGURES_DIR / 'temporal_group_correlation_oneyear.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cache Results for Future Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = PROJ_ROOT / \"results\" / \"xai_temporal\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "importance_evolution.to_csv(results_dir / \"importance_evolution_oneyear.csv\")\n",
    "grouped_importance.to_csv(results_dir / \"grouped_importance_evolution_oneyear.csv\")\n",
    "performance_comparison.to_csv(results_dir / \"performance_by_period_oneyear.csv\", index=False)\n",
    "\n",
    "cache_file = results_dir / \"temporal_cache_oneyear.pkl\"\n",
    "with open(cache_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'period_models': period_models,\n",
    "        'period_predictions': period_predictions,\n",
    "        'period_shap': period_shap,\n",
    "        'ale_evolution': ale_evolution,\n",
    "        'importance_evolution': importance_evolution,\n",
    "        'grouped_importance': grouped_importance,\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}